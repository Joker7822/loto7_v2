import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor
from sklearn.ensemble import (
    GradientBoostingRegressor, RandomForestRegressor, StackingRegressor,
    GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier,
    AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier
)
from genetic_algorithm import evolve_candidates
from sklearn.linear_model import Ridge, LogisticRegression, RidgeClassifier, Perceptron, PassiveAggressiveClassifier, SGDClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.neural_network import MLPRegressor
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.ensemble import StackingRegressor
from statsmodels.tsa.arima.model import ARIMA
from stable_baselines3 import PPO
from gym import spaces
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import optuna
import matplotlib.pyplot as plt
import aiohttp
import asyncio
import warnings
import re
import platform
import gym
import sys
import os
import random
from sklearn.metrics import precision_score, recall_score, f1_score
from neuralforecast.models import TFT
from neuralforecast import NeuralForecast
import onnxruntime
import streamlit as st
from autogluon.tabular import TabularPredictor
import torch.backends.cudnn
from datetime import datetime 
from xgboost import XGBClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import train_test_split
from models.set_transformer import SetTransformer
from stacking_model import (
    train_stacking_model,
    predict_with_stacking,
    convert_number_list_to_vector
)
import traceback  # ä¸Šéƒ¨ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ãŠã„ã¦ãã ã•ã„
import subprocess

# Windowsç’°å¢ƒã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãƒãƒªã‚·ãƒ¼ã‚’è¨­å®š
if platform.system() == "Windows":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

warnings.filterwarnings("ignore")

SEED = int(time.time()) % (2**32)  # å‹•çš„ãªã‚·ãƒ¼ãƒ‰å€¤
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

torch.backends.cudnn.deterministic = False  # å¤šæ§˜æ€§ã‚’å„ªå…ˆã™ã‚‹ãªã‚‰ False
torch.backends.cudnn.benchmark = True       # æ€§èƒ½æœ€é©åŒ–ã‚’æœ‰åŠ¹ã«
def git_commit_and_push(file_path, message):
    try:
        subprocess.run(["git", "add", file_path], check=True)
        diff = subprocess.run(["git", "diff", "--cached", "--quiet"])
        if diff.returncode != 0:
            subprocess.run(["git", "config", "--global", "user.name", "github-actions"], check=True)
            subprocess.run(["git", "config", "--global", "user.email", "github-actions@github.com"], check=True)
            subprocess.run(["git", "commit", "-m", message], check=True)
            subprocess.run(["git", "push"], check=True)
        else:
            print(f"[INFO] No changes in {file_path}")
    except Exception as e:
        print(f"[WARNING] Git commit/push failed: {e}")

# å®Ÿè¡Œå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè¤‡æ•°ï¼‰
targets = [
    "loto7_predictions.csv",
    "loto7_prediction_evaluation_with_bonus.csv",
    "loto7_evaluation_summary.txt",
    "self_predictions.csv"
]

for file in targets:
    git_commit_and_push(file, f"Auto update {file} [skip ci]")

class LotoEnv(gym.Env):
    def __init__(self, historical_numbers):
        super(LotoEnv, self).__init__()
        self.historical_numbers = historical_numbers
        self.action_space = spaces.Box(low=0, high=1, shape=(37,), dtype=np.float32)
        self.observation_space = spaces.Box(low=0, high=1, shape=(37,), dtype=np.float32)

    def reset(self):
        return np.zeros(37, dtype=np.float32)

    def step(self, action):
        action = np.clip(action, 0, 1)
        selected_numbers = np.argsort(action)[-7:] + 1  # ä¸Šä½7å€‹

        # ç›´è¿‘ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æœ¬ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’é¸ã‚“ã§ã€Œå¯¾æˆ¦ã€
        winning_numbers = set(np.random.choice(self.historical_numbers, 7, replace=False))

        main_match = len(set(selected_numbers) & winning_numbers)

        reward = main_match / 7  # æœ¬æ•°å­—ä¸€è‡´æ•°ã§ã‚¹ã‚³ã‚¢
        done = True
        obs = np.zeros(37, dtype=np.float32)

        return obs, reward, done, {}

class LotoGAN(nn.Module):
    def __init__(self, noise_dim=100):
        super(LotoGAN, self).__init__()
        self.generator = nn.Sequential(
            nn.Linear(noise_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 37),
            nn.Sigmoid()
        )
        self.discriminator = nn.Sequential(
            nn.Linear(37, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        self.noise_dim = noise_dim

    def generate_samples(self, num_samples):
        noise = torch.randn(num_samples, self.noise_dim)
        with torch.no_grad():
            samples = self.generator(noise)
        return samples.numpy()

def create_advanced_features(dataframe):
    import numpy as np
    import pandas as pd
    import re
    from itertools import combinations

    def convert_to_number_list(x):
        if isinstance(x, str):
            cleaned = re.sub(r"[^\d\s]", " ", x)
            return [int(n) for n in cleaned.split() if n.isdigit()]
        elif isinstance(x, list):
            return [int(n) for n in x if isinstance(n, (int, float)) and not pd.isna(n)]
        return []

    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    dataframe['æœ¬æ•°å­—'] = dataframe['æœ¬æ•°å­—'].apply(convert_to_number_list)
    dataframe['ãƒœãƒ¼ãƒŠã‚¹æ•°å­—'] = dataframe['ãƒœãƒ¼ãƒŠã‚¹æ•°å­—'].apply(convert_to_number_list)
    dataframe['æŠ½ã›ã‚“æ—¥'] = pd.to_datetime(dataframe['æŠ½ã›ã‚“æ—¥'], errors='coerce')
    dataframe = dataframe.dropna(subset=['æŠ½ã›ã‚“æ—¥'])

    valid_mask = (dataframe['æœ¬æ•°å­—'].apply(len) == 7) & (dataframe['ãƒœãƒ¼ãƒŠã‚¹æ•°å­—'].apply(len) == 2)
    dataframe = dataframe[valid_mask].copy()
    if dataframe.empty:
        print("[ERROR] æœ‰åŠ¹ãªæŠ½ã›ã‚“ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

    # æ•°å­—é…åˆ—å±•é–‹
    try:
        nums_array = np.vstack(dataframe['æœ¬æ•°å­—'].values)
    except Exception as e:
        print(f"[ERROR] æ•°å­—ã®vstackã«å¤±æ•—: {e}")
        return pd.DataFrame()

    sorted_nums = np.sort(nums_array, axis=1)
    diffs = np.diff(sorted_nums, axis=1)

    # ç‰¹å¾´é‡ç”Ÿæˆ
    features = pd.DataFrame(index=dataframe.index)
    features['å¥‡æ•°æ¯”'] = (nums_array % 2 != 0).sum(axis=1) / 7
    features['å¶æ•°æ¯”'] = (nums_array % 2 == 0).sum(axis=1) / 7
    features['æœ¬æ•°å­—åˆè¨ˆ'] = nums_array.sum(axis=1)
    features['ãƒ¬ãƒ³ã‚¸'] = sorted_nums[:, -1] - sorted_nums[:, 0]
    features['æ¨™æº–åå·®'] = np.std(nums_array, axis=1)
    features['ä¸­å¤®å€¤'] = np.median(nums_array, axis=1)
    features['æ•°å­—å¹³å‡'] = np.mean(nums_array, axis=1)
    features['é€£ç•ªæ•°'] = (diffs == 1).sum(axis=1)
    features['æœ€å°é–“éš”'] = diffs.min(axis=1)
    features['æœ€å¤§é–“éš”'] = diffs.max(axis=1)
    features['æ›œæ—¥'] = dataframe['æŠ½ã›ã‚“æ—¥'].dt.dayofweek
    features['æœˆ'] = dataframe['æŠ½ã›ã‚“æ—¥'].dt.month
    features['å¹´'] = dataframe['æŠ½ã›ã‚“æ—¥'].dt.year

    # å‡ºç¾é–“éš”å¹³å‡
    last_seen = {}
    gaps = []
    for idx, nums in dataframe['æœ¬æ•°å­—'].items():
        gap = [idx - last_seen.get(n, idx) for n in nums]
        gaps.append(np.mean(gap))
        for n in nums:
            last_seen[n] = idx
    features['å‡ºç¾é–“éš”å¹³å‡'] = gaps

    # å‡ºç¾é »åº¦ã‚¹ã‚³ã‚¢
    all_numbers = [n for nums in dataframe['æœ¬æ•°å­—'] for n in nums]
    freq_dict = pd.Series(all_numbers).value_counts().to_dict()
    features['å‡ºç¾é »åº¦ã‚¹ã‚³ã‚¢'] = dataframe['æœ¬æ•°å­—'].apply(
        lambda nums: sum(freq_dict.get(n, 0) for n in nums) / len(nums)
    )

    pair_freq = {}
    triple_freq = {}
    quad_freq = {}  # âœ… è¿½åŠ 

    for nums in dataframe['æœ¬æ•°å­—']:
        for pair in combinations(sorted(nums), 2):
            pair_freq[pair] = pair_freq.get(pair, 0) + 1
        for triple in combinations(sorted(nums), 3):
            triple_freq[triple] = triple_freq.get(triple, 0) + 1
        for quad in combinations(sorted(nums), 4):  # âœ… è¿½åŠ 
            quad_freq[quad] = quad_freq.get(quad, 0) + 1

    # --- ç‰¹å¾´é‡åˆ—ã‚’è¿½åŠ  ---
    features['ãƒšã‚¢å‡ºç¾é »åº¦'] = dataframe['æœ¬æ•°å­—'].apply(
        lambda nums: sum(pair_freq.get(tuple(sorted((nums[i], nums[j]))), 0)
                        for i in range(7) for j in range(i+1, 7))
    )

    features['ãƒˆãƒªãƒ—ãƒ«å‡ºç¾é »åº¦'] = dataframe['æœ¬æ•°å­—'].apply(
        lambda nums: sum(triple_freq.get(tuple(sorted((nums[i], nums[j], nums[k]))), 0)
                        for i in range(7) for j in range(i+1, 7) for k in range(j+1, 7))
    )

    features['ã‚¯ãƒ¯ãƒƒãƒ‰å‡ºç¾é »åº¦'] = dataframe['æœ¬æ•°å­—'].apply(  # âœ… è¿½åŠ 
        lambda nums: sum(quad_freq.get(tuple(sorted((nums[i], nums[j], nums[k], nums[l]))), 0)
                        for i in range(7) for j in range(i+1, 7)
                        for k in range(j+1, 7) for l in range(k+1, 7))
    )

    # ç›´è¿‘5å›å‡ºç¾ç‡
    past_5_counts = []
    for i, row in dataframe.iterrows():
        current_date = row['æŠ½ã›ã‚“æ—¥']
        recent = dataframe[dataframe['æŠ½ã›ã‚“æ—¥'] < current_date].tail(5)
        recent_nums = [n for nums in recent['æœ¬æ•°å­—'] for n in nums]
        match_count = sum(n in recent_nums for n in row['æœ¬æ•°å­—'])
        past_5_counts.append(match_count / 7)
    features['ç›´è¿‘5å›å‡ºç¾ç‡'] = past_5_counts

    # åˆ—é †ã‚’å®‰å®šåŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    features = features[sorted(features.columns, key=lambda x: x.encode('utf-8'))]

    # çµåˆã—ã¦è¿”ã™
    return pd.concat([dataframe.reset_index(drop=True), features.reset_index(drop=True)], axis=1)

def preprocess_data(data):
    """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†: ç‰¹å¾´é‡ã®ä½œæˆ & ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""
    
    processed_data = create_advanced_features(data)
    if processed_data.empty:
        print("ã‚¨ãƒ©ãƒ¼: ç‰¹å¾´é‡ç”Ÿæˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None, None, None, None

    print("=== ç‰¹å¾´é‡ä½œæˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ ===")
    print(processed_data.head())

    # æ•°å€¤ç‰¹å¾´é‡ã®é¸æŠ
    numeric_features = processed_data.select_dtypes(include=[np.number]).columns.tolist()
    X = processed_data[numeric_features].fillna(0)

    print(f"æ•°å€¤ç‰¹å¾´é‡ã®æ•°: {len(numeric_features)}, ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}")
    if X.empty:
        print("ã‚¨ãƒ©ãƒ¼: æ•°å€¤ç‰¹å¾´é‡ãŒä½œæˆã•ã‚Œãšã€ãƒ‡ãƒ¼ã‚¿ãŒç©ºã«ãªã£ã¦ã„ã¾ã™ã€‚")
        return None, None, None, None

    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    print("=== ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ ===")
    print(X_scaled[:5])

    # ç›®æ¨™å¤‰æ•°ã®ä½œæˆ
    try:
        y = np.array([list(map(int, nums)) for nums in processed_data['æœ¬æ•°å­—']])
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: ç›®æ¨™å¤‰æ•°ã®ä½œæˆæ™‚ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, None, None, None

    return X_scaled, y, scaler, numeric_features  # â† ç‰¹å¾´é‡åã‚’è¿½åŠ ã—ã¦è¿”ã™

def convert_numbers_to_binary_vectors(data):
    """
    æœ¬æ•°å­—ã‚’0/1ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹
    ä¾‹ï¼š[1,5,7,22,28,30,36] â†’ [1,0,0,0,1,0,1, ..., 0,1]
    """
    vectors = []
    for numbers in data['æœ¬æ•°å­—']:
        vec = np.zeros(37)
        for n in numbers:
            if 1 <= n <= 37:
                vec[n-1] = 1
        vectors.append(vec)
    return np.array(vectors)

def calculate_prediction_errors(predictions, actual_numbers):
    """äºˆæ¸¬å€¤ã¨å®Ÿéš›ã®å½“é¸çµæœã®èª¤å·®ã‚’è¨ˆç®—ã—ã€ç‰¹å¾´é‡ã¨ã—ã¦ä¿å­˜"""
    errors = []
    for pred, actual in zip(predictions, actual_numbers):
        pred_numbers = set(pred[0])
        actual_numbers = set(actual)
        error_count = len(actual_numbers - pred_numbers)
        errors.append(error_count)
    
    return np.mean(errors)

def save_self_predictions(predictions, file_path="self_predictions.csv", max_records=100):
    """äºˆæ¸¬çµæœã‚’CSVã«ä¿å­˜ã—ã€ä¿å­˜ä»¶æ•°ã‚’æœ€å¤§max_recordsã«åˆ¶é™ã—ã€ä¸–ä»£ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜"""

    # ğŸ”’ ç©ºã®äºˆæ¸¬ã¯ä¿å­˜ã—ãªã„
    if not predictions:
        print("[WARNING] ä¿å­˜å¯¾è±¡ã®äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    rows = []
    for numbers, confidence in predictions:
        rows.append(numbers.tolist())

    # --- æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿ ---
    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
        try:
            existing = pd.read_csv(file_path, header=None).values.tolist()
            rows = existing + rows
        except pd.errors.EmptyDataError:
            print(f"[WARNING] {file_path} ãŒç©ºã¾ãŸã¯ç„¡åŠ¹ãªå½¢å¼ã®ãŸã‚ã€æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
    else:
        print(f"[INFO] {file_path} ãŒå­˜åœ¨ã—ãªã„ã‹ç©ºã®ãŸã‚ã€æ–°è¦ä½œæˆã—ã¾ã™ã€‚")

    # æœ€æ–°max_recordsä»¶ã ã‘æ®‹ã™
    rows = rows[-max_records:]
    df = pd.DataFrame(rows)

    # --- ãƒ¡ã‚¤ãƒ³ä¿å­˜ ---
    df.to_csv(file_path, index=False, header=False)
    print(f"[INFO] è‡ªå·±äºˆæ¸¬çµæœã‚’ {file_path} ã«ä¿å­˜ã—ã¾ã—ãŸï¼ˆæœ€å¤§{max_records}ä»¶ï¼‰")

    # --- ä¸–ä»£ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ ---
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = "self_predictions_history"
    os.makedirs(output_dir, exist_ok=True)

    generation_file = os.path.join(output_dir, f"self_predictions_gen_{timestamp}.csv")
    df.to_csv(generation_file, index=False, header=False)
    print(f"[INFO] ä¸–ä»£åˆ¥ã«è‡ªå·±äºˆæ¸¬ã‚‚ä¿å­˜ã—ã¾ã—ãŸ: {generation_file}")

def load_self_predictions(file_path="self_predictions.csv", min_match_threshold=3, true_data=None):
    if not os.path.exists(file_path):
        print(f"[INFO] è‡ªå·±äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    try:
        # ğŸ”¥ é«˜é€Ÿç‰ˆã«ç½®ãæ›ãˆï¼
        df = pd.read_csv(file_path, header=None)
        df = df.dropna()
        df = df[df.apply(lambda row: all(1 <= x <= 37 for x in row), axis=1)]
        numbers_list = df.values.tolist()

        if true_data is not None:
            scores = evaluate_self_predictions(numbers_list, true_data)
            filtered_rows = [r for r, s in zip(numbers_list, scores) if s >= min_match_threshold]
            print(f"[INFO] ä¸€è‡´æ•°{min_match_threshold}ä»¥ä¸Šã®è‡ªå·±äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿: {len(filtered_rows)}ä»¶")
            return filtered_rows
        else:
            return numbers_list

    except Exception as e:
        print(f"[ERROR] è‡ªå·±äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def evaluate_self_predictions(self_predictions, true_data):
    """
    è‡ªå·±äºˆæ¸¬ãƒªã‚¹ãƒˆã¨æœ¬ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒã—ã¦ä¸€è‡´æ•°ã‚’è©•ä¾¡
    :param self_predictions: [[5,12,17,22,30,34,37], ...]
    :param true_data: éå»ã®æœ¬ç‰©æœ¬æ•°å­—ãƒ‡ãƒ¼ã‚¿ï¼ˆdata['æœ¬æ•°å­—'].tolist()ï¼‰
    :return: å„è‡ªå·±äºˆæ¸¬ã«å¯¾å¿œã™ã‚‹æœ€å¤§ä¸€è‡´æ•°ãƒªã‚¹ãƒˆ
    """
    scores = []
    true_sets = [set(nums) for nums in true_data]

    for pred in self_predictions:
        pred_set = set(pred)
        max_match = 0
        for true_set in true_sets:
            match = len(pred_set & true_set)
            if match > max_match:
                max_match = match
        scores.append(max_match)

    return scores

def update_features_based_on_results(data, accuracy_results):
    """éå»ã®äºˆæ¸¬çµæœã¨å®Ÿéš›ã®çµæœã®æ¯”è¼ƒã‹ã‚‰ç‰¹å¾´é‡ã‚’æ›´æ–°"""
    
    for result in accuracy_results:
        event_date = result["æŠ½ã›ã‚“æ—¥"]
        max_matches = result["æœ€é«˜ä¸€è‡´æ•°"]
        avg_matches = result["å¹³å‡ä¸€è‡´æ•°"]
        confidence_avg = result["ä¿¡é ¼åº¦å¹³å‡"]

        # éå»ã®ãƒ‡ãƒ¼ã‚¿ã«äºˆæ¸¬ç²¾åº¦ã‚’çµ„ã¿è¾¼ã‚€
        data.loc[data["æŠ½ã›ã‚“æ—¥"] == event_date, "éå»ã®æœ€å¤§ä¸€è‡´æ•°"] = max_matches
        data.loc[data["æŠ½ã›ã‚“æ—¥"] == event_date, "éå»ã®å¹³å‡ä¸€è‡´æ•°"] = avg_matches
        data.loc[data["æŠ½ã›ã‚“æ—¥"] == event_date, "éå»ã®äºˆæ¸¬ä¿¡é ¼åº¦"] = confidence_avg

    # ç‰¹å¾´é‡ãŒãªã„å ´åˆã¯0ã§åŸ‹ã‚ã‚‹
    data["éå»ã®æœ€å¤§ä¸€è‡´æ•°"] = data["éå»ã®æœ€å¤§ä¸€è‡´æ•°"].fillna(0)
    data["éå»ã®å¹³å‡ä¸€è‡´æ•°"] = data["éå»ã®å¹³å‡ä¸€è‡´æ•°"].fillna(0)
    data["éå»ã®äºˆæ¸¬ä¿¡é ¼åº¦"] = data["éå»ã®äºˆæ¸¬ä¿¡é ¼åº¦"].fillna(0)

    return data

class LotoLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LotoLSTM, self).__init__()
        self.lstm = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)
        self.attn = nn.Linear(hidden_size * 2, 1)
        self.fc = nn.Linear(hidden_size * 2, output_size)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)  # [batch, seq_len, hidden*2]
        attn_weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)  # [batch, seq_len]
        context = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # [batch, hidden*2]
        out = self.fc(context)
        return out

def train_lstm_model(X_train, y_train, input_size, device):
    
    torch.backends.cudnn.benchmark = True  # â˜…ã“ã‚Œã‚’è¿½åŠ 
    
    model = LotoLSTM(input_size=input_size, hidden_size=128, output_size=7).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    dataset = TensorDataset(
        torch.tensor(X_train, dtype=torch.float32),
        torch.tensor(y_train, dtype=torch.float32)
    )
    loader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=2)  # â˜…å¤‰æ›´

    scaler = torch.cuda.amp.GradScaler()  # â˜…Mixed Precisionè¿½åŠ 

    model.train()
    for epoch in range(50):
        total_loss = 0
        for batch_X, batch_y in loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            optimizer.zero_grad()
            with torch.cuda.amp.autocast():  # â˜…ã“ã“ã‚‚Mixed Precision
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            total_loss += loss.item()
        print(f"[LSTM] Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}")

    # ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    dummy_input = torch.randn(1, 1, input_size).to(device)
    torch.onnx.export(
        model,
        dummy_input,
        "lstm_model.onnx",
        input_names=["input"],
        output_names=["output"],
        dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
        opset_version=12
    )
    print("[INFO] LSTM ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†")
    return model

def extract_high_accuracy_combinations(evaluation_df, threshold=6):
    high_matches = evaluation_df[evaluation_df["æœ¬æ•°å­—ä¸€è‡´æ•°"] >= threshold]
    return high_matches

def convert_hit_combos_to_training_data(hit_combos, original_data):
    temp_df = original_data.copy()
    new_rows = []
    for _, row in hit_combos.iterrows():
        temp = {
            "æŠ½ã›ã‚“æ—¥": row["æŠ½ã›ã‚“æ—¥"],
            "æœ¬æ•°å­—": row["äºˆæ¸¬ç•ªå·"],
            "ãƒœãƒ¼ãƒŠã‚¹æ•°å­—": row["å½“é¸ãƒœãƒ¼ãƒŠã‚¹"]
        }
        new_rows.append(temp)
    if not new_rows:
        return None, None
    temp_df = pd.DataFrame(new_rows)
    return preprocess_data(temp_df)[:2]

class LotoPredictor:
    def __init__(self, input_size, hidden_size, output_size):
        print("[INFO] ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–")
        device = torch.device("cpu")
        self.lstm_model = LotoLSTM(input_size, hidden_size, output_size)
        self.regression_models = [None] * 7
        self.scaler = None
        self.onnx_session = None
        self.gan_model = None
        self.ppo_model = None
        self.set_model = SetTransformer().to(device)  # âœ… ã“ã“ã§ä½¿ã£ã¦ã„ã‚‹ device
        self.stacking_model = None

        # --- GANãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰ ---
        if os.path.exists("gan_model.pth"):
            self.gan_model = LotoGAN()
            self.gan_model.load_state_dict(torch.load("gan_model.pth"))
            self.gan_model.eval()
            print("[INFO] GANãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

        # --- PPOãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰ ---
        if os.path.exists("ppo_model.zip"):
            self.ppo_model = PPO.load("ppo_model")
            print("[INFO] PPOãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

    def load_onnx_model(self, onnx_path="lstm_model.onnx"):
        print("[INFO] ONNX ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™")
        self.onnx_session = onnxruntime.InferenceSession(
            onnx_path,
            providers=['CPUExecutionProvider']
        )

    def predict_with_onnx(self, X):
        if self.onnx_session is None:
            print("[ERROR] ONNX ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None

        input_name = self.onnx_session.get_inputs()[0].name
        output = self.onnx_session.run(None, {input_name: X.astype(np.float32)})
        return output[0]

    def train_model(self, data, accuracy_results=None):
        if accuracy_results is not None:
            print("[DEBUG] accuracy_results ã«è¨˜éŒ²ã‚’è¡Œã„ã¾ã™")

        now = pd.Timestamp.now()
        past_data = data[data["æŠ½ã›ã‚“æ—¥"] < now].copy()
        true_numbers = past_data["æœ¬æ•°å­—"].tolist()

        self_data = load_self_predictions(
            file_path="self_predictions.csv",
            min_match_threshold=6,
            true_data=true_numbers
        )
        high_match_combos = extract_high_match_patterns(past_data, min_match=6)

        if self_data or high_match_combos:
            print("[INFO] éå»ã®é«˜ä¸€è‡´è‡ªå·±äºˆæ¸¬ï¼‹é«˜ä¸€è‡´æœ¬ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã™")
            new_rows = []
            for nums in (self_data or []):
                new_rows.append({'æŠ½ã›ã‚“æ—¥': now, 'å›å·': 9999, 'æœ¬æ•°å­—': nums, 'ãƒœãƒ¼ãƒŠã‚¹æ•°å­—': [0, 0]})
            for nums in (high_match_combos or []):
                new_rows.append({'æŠ½ã›ã‚“æ—¥': now, 'å›å·': 9999, 'æœ¬æ•°å­—': nums, 'ãƒœãƒ¼ãƒŠã‚¹æ•°å­—': [0, 0]})
            if new_rows:
                new_data = pd.DataFrame(new_rows)
                past_data = pd.concat([past_data, new_data], ignore_index=True)

        X, y, self.scaler, self.feature_names = preprocess_data(past_data)
        if X is None or y is None or len(X) == 0:
            print("[ERROR] å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return False

        try:
            X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=42)
            input_size = X_train.shape[1]
            device = torch.device("cpu")

            # --- LSTM ãƒ¢ãƒ‡ãƒ« ---
            X_train_tensor = torch.tensor(X_train.reshape(-1, 1, input_size), dtype=torch.float32).to(device)
            y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)
            self.lstm_model = train_lstm_model(X_train_tensor, y_train_tensor, input_size, device)

            # ONNX ä¿å­˜
            dummy_input = torch.randn(1, 1, input_size)
            torch.onnx.export(
                self.lstm_model, dummy_input, "lstm_model.onnx",
                input_names=["input"], output_names=["output"],
                dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
                opset_version=12
            )
            self.load_onnx_model("lstm_model.onnx")
            if self.onnx_session is None:
                print("[ERROR] ONNXãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
            # --- AutoGluon ãƒ¢ãƒ‡ãƒ« ---
            self.regression_models = [None] * 7
            for i in range(7):
                try:
                    df_train = pd.DataFrame(X_train)
                    df_train['target'] = y_train[:, i]
                    predictor = TabularPredictor(label='target', path=f'autogluon_model_pos{i}').fit(
                        df_train,
                        excluded_model_types=['KNN', 'NN_TORCH'],
            hyperparameters={
                'GBM': {'device': 'cpu', 'num_boost_round': 300},
                'XGB': {'tree_method': 'hist', 'n_estimators': 300},
                'CAT': {'task_type': 'CPU', 'iterations': 300},
                'RF': {'n_estimators': 200}
            },
            num_gpus=0
                    )
                    self.regression_models[i] = predictor
                    print(f"[DEBUG] AutoGluon ãƒ¢ãƒ‡ãƒ« {i+1}/7 ã®å­¦ç¿’å®Œäº†")
                except Exception as e:
                    print(f"[ERROR] AutoML ãƒ¢ãƒ‡ãƒ« {i+1} ã®å­¦ç¿’ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                    traceback.print_exc()

            if any(model is None for model in self.regression_models):
                print("[ERROR] ä¸€éƒ¨ã® AutoML ãƒ¢ãƒ‡ãƒ«ãŒæœªå­¦ç¿’ã§ã™")
                return False
            # --- TabNet ãƒ¢ãƒ‡ãƒ« ---
            try:
                from tabnet_module import train_tabnet
                self.tabnet_model = train_tabnet(X_train, y_train)
                print("[INFO] TabNet ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å®Œäº†")
            except Exception as e:
                print(f"[ERROR] TabNet ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«å¤±æ•—: {e}")
                traceback.print_exc()
                self.tabnet_model = None
            # --- SetTransformer ---
            try:
                all_numbers = past_data['æœ¬æ•°å­—'].tolist()
                input_tensor = torch.tensor(all_numbers, dtype=torch.long).to(device)
                label_tensor = torch.tensor(
                    [convert_number_list_to_vector(x) for x in all_numbers],
                    dtype=torch.float32
                ).to(device)

                self.set_model = SetTransformer().to(device)
                optimizer = torch.optim.Adam(self.set_model.parameters(), lr=0.001)
                loss_fn = nn.BCELoss()
                self.set_model.train()
                for epoch in range(300):
                    optimizer.zero_grad()
                    output = self.set_model(input_tensor)
                    loss = loss_fn(output, label_tensor)
                    loss.backward()
                    optimizer.step()
                print("[INFO] SetTransformer ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å®Œäº†")
            except Exception as e:
                print(f"[ERROR] SetTransformer ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«å¤±æ•—: {e}")
                traceback.print_exc()
                return False
            # --- BNN ãƒ¢ãƒ‡ãƒ« ---
            try:
                from bnn_module import train_bayesian_regression
                self.bnn_model, self.bnn_guide = train_bayesian_regression(
                    X_train, y_train, in_features=input_size, out_features=7, num_steps=500
                )
                print("[INFO] BNN ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å®Œäº†")
            except Exception as e:
                print(f"[ERROR] BNN ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«å¤±æ•—: {e}")
                traceback.print_exc()
                self.bnn_model = None
                self.bnn_guide = None
            # --- TFT ãƒ¢ãƒ‡ãƒ« ---
            try:
                from neuralforecast.models import TFT
                from neuralforecast import NeuralForecast
                df_tft = past_data.copy()
                df_tft['ds'] = pd.to_datetime(df_tft['æŠ½ã›ã‚“æ—¥'])
                df_tft['unique_id'] = 'loto'
                df_tft['y'] = df_tft['æœ¬æ•°å­—'].apply(lambda x: sum(x) if isinstance(x, list) else 0)
                df_tft = df_tft[['unique_id', 'ds', 'y']].dropna().sort_values('ds')

                if len(df_tft) < 15:
                    print(f"[WARNING] TFTãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¸è¶³ ({len(df_tft)} ä»¶)ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    self.tft_model = None
                else:
                    self.tft_model = NeuralForecast(models=[TFT(input_size=5, h=1)], freq='W')
                    self.tft_model.fit(df_tft)
                    print("[INFO] TFT ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å®Œäº†")
            except Exception as e:
                print(f"[ERROR] TFT ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«å¤±æ•—: {e}")
                traceback.print_exc()
                self.tft_model = None
            # --- GNN ãƒ¢ãƒ‡ãƒ« ---
            try:
                import networkx as nx
                from torch_geometric.data import Data
                from torch_geometric.nn import GCNConv

                class LotoGNN(nn.Module):
                    def __init__(self):
                        super().__init__()
                        self.conv1 = GCNConv(37, 64)
                        self.conv2 = GCNConv(64, 1)

                    def forward(self, data):
                        x = self.conv1(data.x, data.edge_index).relu()
                        return self.conv2(x, data.edge_index)

                G = nx.Graph()
                for nums in past_data['æœ¬æ•°å­—']:
                    for i in range(len(nums)):
                        for j in range(i + 1, len(nums)):
                            G.add_edge(nums[i] - 1, nums[j] - 1)
                edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous()
                x = torch.eye(37)
                graph_data = Data(x=x, edge_index=edge_index)

                self.gnn_model = LotoGNN().to(device)
                optimizer = torch.optim.Adam(self.gnn_model.parameters(), lr=0.01)
                self.gnn_model.train()
                for epoch in range(100):
                    optimizer.zero_grad()
                    out = self.gnn_model(graph_data.to(device))
                    loss = out.mean()
                    loss.backward()
                    optimizer.step()
                print("[INFO] GNN ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å®Œäº†")
            except Exception as e:
                print(f"[ERROR] GNN ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«å¤±æ•—: {e}")
                traceback.print_exc()
                return False
            # --- Diffusion ãƒ¢ãƒ‡ãƒ«ï¼ˆDDPMï¼‰ ---
            try:
                from diffusion_module import train_diffusion_ddpm
                vectors = [convert_number_list_to_vector(nums) for nums in past_data["æœ¬æ•°å­—"]]
                self.diffusion_model, self.diffusion_betas, self.diffusion_alphas = train_diffusion_ddpm(
                    np.array(vectors), timesteps=100, epochs=500, batch_size=64
                )
                print("[INFO] Diffusion ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å®Œäº†")
            except Exception as e:
                print(f"[ERROR] Diffusion ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«å¤±æ•—: {e}")
                traceback.print_exc()
                self.diffusion_model = None

            # --- stacking ãƒ¢ãƒ‡ãƒ« ---
            try:
                from stacking_model import train_stacking_model

                X_tensor = torch.tensor(X_train.reshape(-1, 1, input_size), dtype=torch.float32).to(device)
                self.lstm_model.eval()
                with torch.no_grad():
                    lstm_preds = self.lstm_model(X_tensor).cpu().numpy().astype(int).tolist()

                automl_preds = []
                for i in range(7):
                    preds = self.regression_models[i].predict(pd.DataFrame(X_train))
                    automl_preds.append(np.round(preds).astype(int).tolist())
                automl_preds = list(map(list, zip(*automl_preds)))

                gan_preds = [self.gan_model.generate_samples(1)[0] if self.gan_model else np.random.rand(37)
                            for _ in range(len(X_train))]
                ppo_preds = []
                for _ in range(len(X_train)):
                    obs = np.zeros(37, dtype=np.float32)
                    action, _ = self.ppo_model.predict(obs, deterministic=True) if self.ppo_model else (np.random.rand(37), None)
                    ppo_preds.append(np.array(action))

                self.stacking_model = train_stacking_model(
                    lstm_preds, automl_preds, gan_preds, ppo_preds, y_train.tolist()
                )
                print("[INFO] stacking_model ã®å­¦ç¿’å®Œäº†")
            except Exception as e:
                print(f"[ERROR] stacking_model ã®å­¦ç¿’ã«å¤±æ•—: {e}")
                traceback.print_exc()
                return False
            # --- Stacking æœ€é©åŒ–ï¼ˆOptunaï¼‰ ---
            try:
                from stacking_optuna import optimize_stacking
                pred_dict = {
                    "lstm": lstm_preds,
                    "automl": automl_preds,
                    "gan": [list(g) for g in gan_preds],
                    "ppo": [list(p) for p in ppo_preds],
                }
                self.best_stacking_weights = optimize_stacking(pred_dict, y_train.tolist())
                print(f"[INFO] Stacking é‡ã¿æœ€é©åŒ–å®Œäº†: {self.best_stacking_weights}")
            except Exception as e:
                print(f"[ERROR] Stacking é‡ã¿æœ€é©åŒ–ã«å¤±æ•—: {e}")
                traceback.print_exc()
                self.best_stacking_weights = None

            return True

        except Exception as e:
            print(f"[ERROR] ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­ã«ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            traceback.print_exc()
            return False

    def predict(self, latest_data, num_candidates=50):
        print(f"[INFO] äºˆæ¸¬ã‚’é–‹å§‹ï¼ˆå€™è£œæ•°: {num_candidates}ï¼‰")
        X, _, _, _ = preprocess_data(latest_data)
    
        if X is None or len(X) == 0:
            print("[ERROR] äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return None, None
    
        if not self.regression_models or any(m is None for m in self.regression_models):
            print("[ERROR] å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆAutoMLï¼‰ãŒæœªå­¦ç¿’ã§ã™")
            return None, None
    
        if self.onnx_session is None:
            print("[ERROR] ONNXãƒ¢ãƒ‡ãƒ«ï¼ˆLSTMï¼‰ãŒæœªãƒ­ãƒ¼ãƒ‰ã§ã™")
            return None, None
    
        if self.gnn_model is None:
            print("[ERROR] GNNãƒ¢ãƒ‡ãƒ«ãŒæœªå­¦ç¿’ã§ã™")
            return None, None
    
        if self.stacking_model is None:
            print("[ERROR] stacking_model ãŒæœªã‚»ãƒƒãƒˆã§ã™")
            return None, None
    
        print(f"[DEBUG] äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿ã® shape: {X.shape}")
        try:
            latest_draw_date = latest_data['æŠ½ã›ã‚“æ—¥'].max()
            past_data = latest_data[latest_data['æŠ½ã›ã‚“æ—¥'] < latest_draw_date]
            if past_data.empty:
                past_data = latest_data.copy()
    
            freq_score = calculate_number_frequencies(past_data)
            cycle_score = calculate_number_cycle_score(past_data)
    
            # --- GNNã‚¹ã‚³ã‚¢è¨ˆç®— ---
            import networkx as nx
            from torch_geometric.data import Data
            G = nx.Graph()
            for nums in past_data['æœ¬æ•°å­—']:
                for i in range(len(nums)):
                    for j in range(i + 1, len(nums)):
                        G.add_edge(nums[i] - 1, nums[j] - 1)
            edge_index = torch.tensor(list(G.edges), dtype=torch.long).t().contiguous()
            x = torch.eye(37)
            graph_data = Data(x=x, edge_index=edge_index)
    
            device = torch.device("cpu")
            self.gnn_model.eval()
            with torch.no_grad():
                gnn_scores = self.gnn_model(graph_data.to(device)).squeeze().cpu().numpy()
    
            # --- ç‰¹å¾´é‡æ•´å½¢ ---
            expected_features = self.regression_models[0].feature_metadata.get_features()
            X_df = pd.DataFrame(X, columns=self.feature_names)
    
            missing_cols = [col for col in expected_features if col not in X_df.columns]
            if missing_cols:
                print(f"[WARNING] AutoMLç”¨ã®ä¸è¶³ç‰¹å¾´é‡ã‚’0ã§è£œå®Œ: {missing_cols}")
                for col in missing_cols:
                    X_df[col] = 0.0
            X_df = X_df[expected_features]
    
            lstm_input_size = self.lstm_model.lstm.input_size
            if X_df.shape[1] < lstm_input_size:
                print(f"[WARNING] LSTMå‘ã‘ç‰¹å¾´é‡ãŒä¸è¶³: {X_df.shape[1]} â†’ {lstm_input_size} ã«è£œå®Œ")
                for i in range(lstm_input_size - X_df.shape[1]):
                    X_df[f'_pad_{i}'] = 0.0
            elif X_df.shape[1] > lstm_input_size:
                print(f"[WARNING] LSTMå‘ã‘ç‰¹å¾´é‡ãŒå¤šã™ãã¾ã™: {X_df.shape[1]} â†’ {lstm_input_size} ã«åˆ‡ã‚Šè©°ã‚")
                X_df = X_df.iloc[:, :lstm_input_size]
    
            # --- GAãƒ™ãƒ¼ã‚¹å€™è£œç”Ÿæˆ ---
            from genetic_algorithm import evolve_candidates
            base_candidates = evolve_candidates(self, X_df, generations=10, population_size=num_candidates)
    
            if not base_candidates or not isinstance(base_candidates, list) or len(base_candidates) < 1:
                print("[ERROR] å€™è£œç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆbase_candidatesãŒç©ºã¾ãŸã¯ä¸æ­£ï¼‰")
                return None, None
    
            all_predictions = []
            for idx, (lstm_vec, automl_vec, gan_vec, ppo_vec) in enumerate(base_candidates):
                try:
                    numbers, _ = predict_with_stacking(self.stacking_model, lstm_vec, automl_vec, gan_vec, ppo_vec)
    
                    flat_numbers = []
                    for n in numbers:
                        if isinstance(n, list):
                            flat_numbers.extend(n)
                        else:
                            flat_numbers.append(n)
    
                    # --- å€™è£œã‚’ã‚¯ãƒªãƒ¼ãƒ³ã«æ•´å½¢ ---
                    flat_numbers = list(set(np.round(flat_numbers).astype(int)))
                    flat_numbers = [n for n in flat_numbers if 1 <= n <= 37]
                    flat_numbers = sorted(flat_numbers)[:7]  # ä¸Šä½7å€‹ã®ã¿
    
                    if len(flat_numbers) != 7:
                        continue  # âŒ ä¸æ­£ãªå€™è£œã¯é™¤å¤–
    
                    # --- ã‚¹ã‚³ã‚¢è¨ˆç®— ---
                    score_freq = sum(freq_score.get(n, 0) for n in flat_numbers)
                    score_cycle = sum(cycle_score.get(n, 0) for n in flat_numbers)
                    score_gnn = sum(gnn_scores[n - 1] for n in flat_numbers)
                    score = score_freq - score_cycle + score_gnn
    
                    if np.isnan(score) or np.isinf(score):
                        continue  # ç•°å¸¸å€¤ã‚¹ã‚­ãƒƒãƒ—
    
                    confidence = 1.0 + (score / 500.0)
                    all_predictions.append((flat_numbers, confidence))
    
                except Exception as e:
                    print(f"[WARNING] stackingäºˆæ¸¬ä¸­ã«ã‚¨ãƒ©ãƒ¼ (index {idx}): {e}")
                    traceback.print_exc()
                    continue
    
            print(f"[INFO] ç·äºˆæ¸¬å€™è£œæ•°: {len(all_predictions)} ä»¶")
    
            if not all_predictions:
                print("[WARNING] æœ‰åŠ¹ãªäºˆæ¸¬å€™è£œãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                return None, None
    
            numbers_only = [pred[0] for pred in all_predictions]
            confidence_scores = [pred[1] for pred in all_predictions]
            return numbers_only, confidence_scores
    
        except Exception as e:
            print(f"[ERROR] äºˆæ¸¬ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            traceback.print_exc()
            return None, None

# äºˆæ¸¬çµæœã®è©•ä¾¡
def evaluate_predictions(predictions, actual_numbers):
    matches = []
    for pred in predictions:
        match_count = len(set(pred[0]) & set(actual_numbers))
        matches.append(match_count)
    return {
        'max_matches': max(matches),
        'avg_matches': np.mean(matches),
        'predictions_with_matches': list(zip(predictions, matches))
    }
# è¿½åŠ : æœ€æ–°ã®æŠ½ã›ã‚“æ—¥ã‚’å–å¾—ã™ã‚‹é–¢æ•°
official_url = "https://www.takarakuji-official.jp/ec/loto7/?kujiprdShbt=61&knyschm=0"

async def fetch_drawing_dates():
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(official_url, timeout=10) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    drawing_dates = []
                    date_elements = soup.select("dl.m_param.m_thumbSet_row")
                    for dl in date_elements:
                        dt_element = dl.find("dt", string="æŠ½ã›ã‚“æ—¥")
                        if dt_element:
                            dd_element = dt_element.find_next_sibling("dd")
                            if dd_element:
                                formatted_date = dd_element.text.strip().replace("/", "-")
                                drawing_dates.append(formatted_date)
                    
                    return drawing_dates
                else:
                    print(f"HTTPã‚¨ãƒ©ãƒ¼ {response.status}: {official_url}")
        except Exception as e:
            print(f"æŠ½ã›ã‚“æ—¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    return []

async def get_latest_drawing_dates():
    dates = await fetch_drawing_dates()
    return dates

def parse_number_string(number_str):
    """
    äºˆæ¸¬ç•ªå·ã‚„å½“é¸ç•ªå·ã®æ–‡å­—åˆ—ã‚’ãƒªã‚¹ãƒˆåŒ–ã™ã‚‹é–¢æ•°
    - ã‚¹ãƒšãƒ¼ã‚¹ / ã‚«ãƒ³ãƒ / ã‚¿ãƒ– åŒºåˆ‡ã‚Šã«å¯¾å¿œ
    - "07 15 20 28 29 34 36" â†’ [7, 15, 20, 28, 29, 34, 36]
    - "[7, 15, 20, 28, 29, 34, 36]" â†’ [7, 15, 20, 28, 29, 34, 36]
    """
    if pd.isna(number_str):
        return []  # NaN ã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
    
    # ä¸è¦ãªè¨˜å·ã‚’å‰Šé™¤ï¼ˆãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆï¼‰
    number_str = number_str.strip("[]").replace("'", "").replace('"', '')

    # ã‚¹ãƒšãƒ¼ã‚¹ãƒ»ã‚«ãƒ³ãƒãƒ»ã‚¿ãƒ–ã§åˆ†å‰²ã—ã€æ•´æ•°å¤‰æ›
    numbers = re.split(r'[\s,]+', number_str)

    # æ•°å­—ã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦æ•´æ•°å¤‰æ›
    return [int(n) for n in numbers if n.isdigit()]

def classify_rank(main_match, bonus_match):
    """æœ¬æ•°å­—ä¸€è‡´æ•°ã¨ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°ã‹ã‚‰Loto7ã®ç­‰ç´šã‚’åˆ¤å®š"""
    if main_match == 7:
        return "1ç­‰"
    elif main_match == 6 and bonus_match >= 1:
        return "2ç­‰"
    elif main_match == 6:
        return "3ç­‰"
    elif main_match == 5:
        return "4ç­‰"
    elif main_match == 4:
        return "5ç­‰"
    elif main_match == 3 and bonus_match >= 1:
        return "6ç­‰"
    else:
        return "è©²å½“ãªã—"
    
def calculate_precision_recall_f1(evaluation_df):
    y_true = []
    y_pred = []

    for _, row in evaluation_df.iterrows():
        actual = set(row["å½“é¸æœ¬æ•°å­—"])
        predicted = set(row["äºˆæ¸¬ç•ªå·"])
        for n in range(1, 38):
            y_true.append(1 if n in actual else 0)
            y_pred.append(1 if n in predicted else 0)

    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    print("\n=== è©•ä¾¡æŒ‡æ¨™ ===")
    print(f"Precision: {precision:.3f}")
    print(f"Recall:    {recall:.3f}")
    print(f"F1 Score:  {f1:.3f}")

def evaluate_prediction_accuracy_with_bonus(predictions_file="loto7_predictions.csv", results_file="loto7.csv"):
    """
    äºˆæ¸¬çµæœã¨å®Ÿéš›ã®å½“é¸çµæœã‚’æ¯”è¼ƒã—ã€ãƒœãƒ¼ãƒŠã‚¹æ•°å­—ã‚’è€ƒæ…®ã—ã¦ç²¾åº¦ã‚’è©•ä¾¡ã—ã€ç­‰ç´šã‚‚åˆ¤å®šã™ã‚‹
    """
    try:
        # === âœ… ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚„æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã§ã‚‚å®‰å…¨ã«èª­ã¿è¾¼ã‚€ ===
        try:
            predictions_df = pd.read_csv(predictions_file, encoding='utf-8-sig')
            if predictions_df.empty or predictions_df.shape[0] == 0 or "æŠ½ã›ã‚“æ—¥" not in predictions_df.columns:
                print(f"[WARNING] äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã‹ç„¡åŠ¹ã§ã™: {predictions_file}")
                return None
        except Exception as read_err:
            print(f"[WARNING] äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å¤±æ•—: {read_err}")
            return None

        results_df = pd.read_csv(results_file, encoding='utf-8-sig')
        evaluation_results = []

        for index, row in predictions_df.iterrows():
            draw_date = row["æŠ½ã›ã‚“æ—¥"]
            actual_row = results_df[results_df["æŠ½ã›ã‚“æ—¥"] == draw_date]
            if actual_row.empty:
                continue

            actual_numbers = parse_number_string(actual_row.iloc[0]["æœ¬æ•°å­—"])
            actual_bonus = parse_number_string(actual_row.iloc[0]["ãƒœãƒ¼ãƒŠã‚¹æ•°å­—"])

            for i in range(1, 6):  # äºˆæ¸¬1ã€œ5
                pred_col = f"äºˆæ¸¬{i}"
                if pred_col not in row or pd.isna(row[pred_col]):
                    continue

                try:
                    predicted_numbers = set(parse_number_string(row[pred_col]))
                    main_match = len(predicted_numbers & set(actual_numbers))
                    bonus_match = len(predicted_numbers & set(actual_bonus))
                    rank = classify_rank(main_match, bonus_match)

                    evaluation_results.append({
                        "æŠ½ã›ã‚“æ—¥": draw_date,
                        "äºˆæ¸¬ç•ªå·": list(predicted_numbers),
                        "å½“é¸æœ¬æ•°å­—": actual_numbers,
                        "å½“é¸ãƒœãƒ¼ãƒŠã‚¹": actual_bonus,
                        "æœ¬æ•°å­—ä¸€è‡´æ•°": main_match,
                        "ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°": bonus_match,
                        "ä¿¡é ¼åº¦": row.get(f"ä¿¡é ¼åº¦{i}", None),
                        "ç­‰ç´š": rank
                    })

                except Exception as e:
                    print(f"äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼ (è¡Œ {index}, äºˆæ¸¬ {i}): {e}")
                    continue

        evaluation_df = pd.DataFrame(evaluation_results)
        evaluation_df.to_csv("loto7_prediction_evaluation_with_bonus.csv", index=False, encoding='utf-8-sig')
        print("äºˆæ¸¬ç²¾åº¦ã®è©•ä¾¡çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: loto7_prediction_evaluation_with_bonus.csv")

        # çµ±è¨ˆå‡ºåŠ›
        print("\n=== äºˆæ¸¬ç²¾åº¦ã®çµ±è¨ˆæƒ…å ± ===")
        if not evaluation_df.empty:
            print(f"æœ€å¤§æœ¬æ•°å­—ä¸€è‡´æ•°: {evaluation_df['æœ¬æ•°å­—ä¸€è‡´æ•°'].max()}")
            print(f"å¹³å‡æœ¬æ•°å­—ä¸€è‡´æ•°: {evaluation_df['æœ¬æ•°å­—ä¸€è‡´æ•°'].mean():.2f}")
            print(f"æœ€å¤§ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°: {evaluation_df['ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°'].max()}")
            print(f"å¹³å‡ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°: {evaluation_df['ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°'].mean():.2f}")
            print("\n--- ç­‰ç´šã®åˆ†å¸ƒ ---")
            print(evaluation_df['ç­‰ç´š'].value_counts())
            # âœ… è©•ä¾¡æŒ‡æ¨™ã‚’è¿½åŠ ã§è¡¨ç¤º
            calculate_precision_recall_f1(evaluation_df)
        else:
            print("è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                # === è©•ä¾¡çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ› ===
        try:
            with open("loto7_evaluation_summary.txt", "w", encoding="utf-8") as f:
                f.write("=== äºˆæ¸¬ç²¾åº¦ã®çµ±è¨ˆæƒ…å ± ===\n")
                f.write(f"æœ€å¤§æœ¬æ•°å­—ä¸€è‡´æ•°: {evaluation_df['æœ¬æ•°å­—ä¸€è‡´æ•°'].max()}\n")
                f.write(f"å¹³å‡æœ¬æ•°å­—ä¸€è‡´æ•°: {evaluation_df['æœ¬æ•°å­—ä¸€è‡´æ•°'].mean():.2f}\n")
                f.write(f"æœ€å¤§ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°: {evaluation_df['ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°'].max()}\n")
                f.write(f"å¹³å‡ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°: {evaluation_df['ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°'].mean():.2f}\n\n")

                f.write("--- ç­‰ç´šã®åˆ†å¸ƒ ---\n")
                f.write(f"{evaluation_df['ç­‰ç´š'].value_counts().to_string()}\n\n")

                # Precision/Recall/F1
                y_true = []
                y_pred = []

                for _, row in evaluation_df.iterrows():
                    actual = set(row["å½“é¸æœ¬æ•°å­—"])
                    predicted = set(row["äºˆæ¸¬ç•ªå·"])
                    for n in range(1, 38):
                        y_true.append(1 if n in actual else 0)
                        y_pred.append(1 if n in predicted else 0)

                precision = precision_score(y_true, y_pred)
                recall = recall_score(y_true, y_pred)
                f1 = f1_score(y_true, y_pred)

                f.write("=== è©•ä¾¡æŒ‡æ¨™ ===\n")
                f.write(f"Precision: {precision:.3f}\n")
                f.write(f"Recall:    {recall:.3f}\n")
                f.write(f"F1 Score:  {f1:.3f}\n")
            print("è©•ä¾¡çµæœã‚’ loto7_evaluation_summary.txt ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"[WARNING] ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å¤±æ•—: {e}")

        return evaluation_df

    except Exception as e:
        print(f"äºˆæ¸¬ç²¾åº¦ã®è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# äºˆæ¸¬çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹é–¢æ•°
def save_predictions_to_csv(predictions, drawing_date, filename="loto7_predictions.csv"):
    row = {"æŠ½ã›ã‚“æ—¥": drawing_date}

    for i, (numbers, confidence) in enumerate(predictions[:5], 1):
        row[f"äºˆæ¸¬{i}"] = ', '.join(map(str, numbers))
        row[f"ä¿¡é ¼åº¦{i}"] = round(confidence, 3)

    df = pd.DataFrame([row])

    if os.path.exists(filename):
        try:
            existing_df = pd.read_csv(filename, encoding='utf-8-sig')
            if "æŠ½ã›ã‚“æ—¥" not in existing_df.columns:
                print(f"è­¦å‘Š: CSVã«'æŠ½ã›ã‚“æ—¥'åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
                existing_df = pd.DataFrame(columns=["æŠ½ã›ã‚“æ—¥"] + [f"äºˆæ¸¬{i}" for i in range(1, 6)] + [f"ä¿¡é ¼åº¦{i}" for i in range(1, 6)])
            existing_df = existing_df[existing_df["æŠ½ã›ã‚“æ—¥"] != drawing_date]
            df = pd.concat([existing_df, df], ignore_index=True)
        except Exception as e:
            print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
            df = pd.DataFrame([row])

    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"äºˆæ¸¬çµæœã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

def is_running_with_streamlit():
    try:
        from streamlit.runtime.scriptrunner import get_script_run_ctx
        return get_script_run_ctx() is not None
    except ImportError:
        return False

def main_with_improved_predictions():
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    drawing_dates = asyncio.run(get_latest_drawing_dates())
    latest_drawing_date = drawing_dates[0] if drawing_dates else "ä¸æ˜"
    print("æœ€æ–°ã®æŠ½ã›ã‚“æ—¥:", latest_drawing_date)

    try:
        data = pd.read_csv("loto7.csv")
        print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    accuracy_results = evaluate_prediction_accuracy_with_bonus("loto7_predictions.csv", "loto7.csv")
    if accuracy_results is not None and not accuracy_results.empty:
        print("éå»ã®äºˆæ¸¬ç²¾åº¦ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚")

    X, _, _ = preprocess_data(data)
    input_size = X.shape[1] if X is not None else 10
    hidden_size = 128
    output_size = 7

    predictor = LotoPredictor(input_size, hidden_size, output_size)

    try:
        print("ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹...")
        predictor.train_model(data)
        print("ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å®Œäº†")
    except Exception as e:
        print(f"ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
        return

    if is_running_with_streamlit():
        st.title("ãƒ­ãƒˆ7äºˆæ¸¬AI")
        if st.button("äºˆæ¸¬ã‚’å®Ÿè¡Œ"):
            try:
                latest_data = data.tail(10)
                predictions, confidence_scores = predictor.predict(latest_data)

                if predictions is None:
                    print("[ERROR] äºˆæ¸¬ã«å¤±æ•—ã—ãŸãŸã‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
                    return  # â¬…ï¸ ã“ã“ã§å¼·åˆ¶çµ‚äº†ã•ã›ã‚‹ã¨å®‰å…¨

                verified_predictions = verify_predictions(list(zip(predictions, confidence_scores)), data)

                for i, (numbers, confidence) in enumerate(verified_predictions[:5], 1):
                    st.write(f"äºˆæ¸¬ {i}: {numbers} (ä¿¡é ¼åº¦: {confidence:.3f})")

                save_predictions_to_csv(verified_predictions, latest_drawing_date)

            except Exception as e:
                st.error(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        print("[INFO] Streamlitä»¥å¤–ã®å®Ÿè¡Œç’°å¢ƒæ¤œå‡ºã€‚é€šå¸¸ã®ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ã§äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        try:
            latest_data = data.tail(10)
            predictions, confidence_scores = predictor.predict(latest_data)

            if predictions is None:
                print("[ERROR] äºˆæ¸¬ã«å¤±æ•—ã—ãŸãŸã‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
                return  # â¬…ï¸ ã“ã“ã§å¼·åˆ¶çµ‚äº†ã•ã›ã‚‹ã¨å®‰å…¨

            verified_predictions = verify_predictions(list(zip(predictions, confidence_scores)), data)

            print("\n=== äºˆæ¸¬çµæœ ===")
            for i, (numbers, confidence) in enumerate(verified_predictions[:5], 1):
                print(f"äºˆæ¸¬ {i}: {numbers} (ä¿¡é ¼åº¦: {confidence:.3f})")

            save_predictions_to_csv(verified_predictions, latest_drawing_date)

        except Exception as e:
            print(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
    
def calculate_pattern_score(numbers, historical_data=None):
    score = 0
    odd_count = sum(1 for n in numbers if n % 2 != 0)
    if 2 <= odd_count <= 5:
        score += 1
    gaps = [numbers[i+1] - numbers[i] for i in range(len(numbers)-1)]
    if min(gaps) >= 2:
        score += 1
    total = sum(numbers)
    if 100 <= total <= 150:
        score += 1
    if max(numbers) - min(numbers) >= 15:
        score += 1
    return score

def plot_prediction_analysis(predictions, historical_data):
    plt.figure(figsize=(15, 10))
    
    # äºˆæ¸¬ç•ªå·ã®åˆ†å¸ƒ
    plt.subplot(2, 2, 1)
    all_predicted_numbers = [num for pred in predictions for num in pred[0]]
    plt.hist(all_predicted_numbers, bins=37, range=(1, 38), alpha=0.7)
    plt.title('äºˆæ¸¬ç•ªå·ã®åˆ†å¸ƒ')
    plt.xlabel('æ•°å­—')
    plt.ylabel('é »åº¦')
    
    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
    plt.subplot(2, 2, 2)
    confidence_scores = [pred[1] for pred in predictions]
    plt.hist(confidence_scores, bins=20, alpha=0.7)
    plt.title('ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ')
    plt.xlabel('ä¿¡é ¼åº¦')
    plt.ylabel('é »åº¦')
    
    # éå»ã®å½“é¸ç•ªå·ã¨ã®æ¯”è¼ƒ
    plt.subplot(2, 2, 3)
    historical_numbers = [num for numbers in historical_data['æœ¬æ•°å­—'] for num in numbers]
    plt.hist(historical_numbers, bins=37, range=(1, 38), alpha=0.5, label='éå»ã®å½“é¸')
    plt.hist(all_predicted_numbers, bins=37, range=(1, 38), alpha=0.5, label='äºˆæ¸¬')
    plt.title('äºˆæ¸¬ vs éå»ã®å½“é¸')
    plt.xlabel('æ•°å­—')
    plt.ylabel('é »åº¦')
    plt.legend()
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    plt.subplot(2, 2, 4)
    pattern_scores = [calculate_pattern_score(pred[0]) for pred in predictions]
    plt.scatter(range(len(pattern_scores)), pattern_scores, alpha=0.5)
    plt.title('äºˆæ¸¬ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚³ã‚¢')
    plt.xlabel('äºˆæ¸¬ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹')
    plt.ylabel('ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚³ã‚¢')
    
    plt.tight_layout()
    plt.savefig('prediction_analysis.png')
    plt.close()

def generate_evolution_graph(log_file="evolution_log.txt", output_file="evolution_graph.png"):
    """
    evolution_log.txtã‚’èª­ã¿è¾¼ã‚“ã§é€²åŒ–ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆãƒ»ä¿å­˜ã™ã‚‹
    """
    if not os.path.exists(log_file):
        print(f"[WARNING] é€²åŒ–ãƒ­ã‚° {log_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    dates = []
    counts = []

    with open(log_file, "r", encoding="utf-8") as f:
        for line in f:
            try:
                parts = line.strip().split(":")
                date_part = parts[0].strip()
                count_part = parts[2].strip()

                date = pd.to_datetime(date_part)
                count = int(count_part.split()[0])

                dates.append(date)
                counts.append(count)
            except Exception as e:
                print(f"[WARNING] ãƒ­ã‚°ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}")
                continue

    if not dates:
        print("[WARNING] é€²åŒ–ãƒ­ã‚°ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # --- ã‚°ãƒ©ãƒ•æç”» ---
    plt.figure(figsize=(10, 6))
    plt.plot(dates, counts, marker='o', linestyle='-', color='blue')
    plt.title("è‡ªå·±é€²åŒ–å±¥æ­´ï¼ˆè‡ªå·±äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ä»¶æ•°æ¨ç§»ï¼‰")
    plt.xlabel("æ—¥æ™‚")
    plt.ylabel("è‡ªå·±äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ä»¶æ•°")
    plt.grid(True)
    plt.tight_layout()

    # --- ä¿å­˜ ---
    plt.savefig(output_file)
    plt.close()
    print(f"[INFO] é€²åŒ–å±¥æ­´ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")

def verify_predictions(predictions, historical_data, top_k=5):
    def check_number_constraints(numbers):
        """äºˆæ¸¬æ•°å­—é…åˆ—ã®åˆ¶ç´„ãƒã‚§ãƒƒã‚¯"""
        if len(numbers) != 7:
            return False
        if len(np.unique(numbers)) != 7:
            return False
        if not np.all((numbers >= 1) & (numbers <= 37)):
            return False
        if not np.issubdtype(numbers.dtype, np.integer):
            return False
        return True

    print(f"[INFO] äºˆæ¸¬å€™è£œã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...ï¼ˆç·æ•°: {len(predictions)}ï¼‰")

    # --- æœ‰åŠ¹ãªäºˆæ¸¬ã ã‘æŠ½å‡º ---
    valid_predictions = []
    for pred, conf in predictions:
        try:
            numbers = np.array(pred)
            numbers = np.unique(np.round(numbers).astype(int))
            numbers = np.sort(numbers)
            if check_number_constraints(numbers):
                valid_predictions.append((numbers, conf))
        except Exception as e:
            print(f"[WARNING] äºˆæ¸¬æ•´å½¢ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    print(f"[INFO] æœ‰åŠ¹ãªäºˆæ¸¬æ•°: {len(valid_predictions)} ä»¶")

    if not valid_predictions:
        print("[WARNING] æœ‰åŠ¹ãªäºˆæ¸¬ãŒã‚ã‚Šã¾ã›ã‚“")
        return []

    # --- ä¸Šä½å€™è£œã‚’é¸å®š ---
    valid_predictions.sort(key=lambda x: x[1], reverse=True)
    candidates = valid_predictions[:max(100, top_k)]

    # --- ã‚«ãƒãƒ¬ãƒƒã‚¸æœ€å¤§åŒ–ã«ã‚ˆã‚‹é¸æŠœ ---
    selected = []
    used_numbers = set()
    used_flags = [False] * len(candidates)

    while len(selected) < max(top_k - 2, 1):
        best_score = -1
        best_idx = -1

        for idx, (numbers_set, conf) in enumerate(candidates):
            if used_flags[idx]:
                continue
            combined = used_numbers.union(numbers_set)
            coverage_score = len(combined)
            random_boost = random.uniform(0, 1) * 0.1
            total_score = (coverage_score * 0.6) + (conf * 0.2) + random_boost

            if total_score > best_score:
                best_score = total_score
                best_idx = idx

        if best_idx == -1:
            break

        selected.append(candidates[best_idx])
        used_numbers.update(candidates[best_idx][0])
        used_flags[best_idx] = True

    # --- å¼·åˆ¶6æœ¬æ§‹æˆã‚’è¿½åŠ  ---
    try:
        historical = historical_data.copy()
        historical['æœ¬æ•°å­—'] = historical['æœ¬æ•°å­—'].apply(lambda x: list(map(int, x)) if isinstance(x, list) else [])

        high_match_rows = []
        for idx1, row1 in historical.iterrows():
            nums1 = set(row1['æœ¬æ•°å­—'])
            for idx2, row2 in historical.iterrows():
                if idx1 >= idx2:
                    continue
                nums2 = set(row2['æœ¬æ•°å­—'])
                if len(nums1 & nums2) >= 6:
                    high_match_rows.append(list(nums1))

        if high_match_rows:
            added_templates = set()
            attempts = 0
            while len(added_templates) < 2 and attempts < 10:
                template = tuple(sorted(random.choice(high_match_rows)))
                if template in added_templates:
                    attempts += 1
                    continue
                template_set = set(template)
                available_numbers = list(set(range(1, 38)) - template_set)
                if available_numbers:
                    removed = random.choice(list(template_set))
                    added = random.choice(available_numbers)
                    template_set.remove(removed)
                    template_set.add(added)
                final_combo = sorted(template_set)
                selected.append((np.array(final_combo), 1.0))
                added_templates.add(template)
            print("[INFO] å¼·åˆ¶6æœ¬æ§‹æˆã‚’è¿½åŠ ã—ã¾ã—ãŸ")
        else:
            print("[WARNING] éå»ã«6æœ¬ä¸€è‡´ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    except Exception as e:
        print(f"[WARNING] å¼·åˆ¶æ§‹æˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

    print("[INFO] æœ€çµ‚é¸æŠã•ã‚ŒãŸäºˆæ¸¬æ•°:", len(selected))
    return selected
# --- ğŸ”¥ æ–°è¦è¿½åŠ é–¢æ•° ---
def extract_high_match_patterns(dataframe, min_match=6):
    """éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é«˜ä¸€è‡´ãƒ‘ã‚¿ãƒ¼ãƒ³ã ã‘æŠ½å‡º"""
    high_match_combos = []
    for idx1, row1 in dataframe.iterrows():
        nums1 = set(row1['æœ¬æ•°å­—'])
        for idx2, row2 in dataframe.iterrows():
            if idx1 >= idx2:
                continue
            nums2 = set(row2['æœ¬æ•°å­—'])
            if len(nums1 & nums2) >= min_match:
                high_match_combos.append(sorted(nums1))
    return high_match_combos

def calculate_number_frequencies(dataframe):
    """éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç•ªå·å‡ºç¾é »åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    all_numbers = [num for nums in dataframe['æœ¬æ•°å­—'] for num in nums]
    freq = pd.Series(all_numbers).value_counts().to_dict()
    return freq

def calculate_number_cycle_score(dataframe):
    """ç•ªå·ã”ã¨ã®å‡ºç¾å‘¨æœŸã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    number_last_seen = {n: None for n in range(1, 38)}
    number_cycle = {n: [] for n in range(1, 38)}
    
    for i, nums in enumerate(dataframe['æœ¬æ•°å­—']):
        for n in range(1, 38):
            if n in nums:
                if number_last_seen[n] is not None:
                    cycle = i - number_last_seen[n]
                    number_cycle[n].append(cycle)
                number_last_seen[n] = i

    avg_cycle = {n: np.mean(cycles) if cycles else 999 for n, cycles in number_cycle.items()}
    return avg_cycle

def bulk_predict_all_past_draws():
    df = pd.read_csv("loto7.csv")
    df["æŠ½ã›ã‚“æ—¥"] = pd.to_datetime(df["æŠ½ã›ã‚“æ—¥"])
    df = df.sort_values("æŠ½ã›ã‚“æ—¥").reset_index(drop=True)

    predictions_file = "loto7_predictions.csv"

    # âœ… æ—¢å­˜ã®äºˆæ¸¬æ—¥ä»˜ã‚’èª­ã¿å–ã‚Š
    predicted_dates = set()
    if os.path.exists(predictions_file):
        try:
            pred_df = pd.read_csv(predictions_file, encoding='utf-8-sig')
            pred_df["æŠ½ã›ã‚“æ—¥"] = pd.to_datetime(pred_df["æŠ½ã›ã‚“æ—¥"], errors='coerce')
            predicted_dates = set(pred_df["æŠ½ã›ã‚“æ—¥"].dropna().dt.date)
            print(f"[INFO] äºˆæ¸¬æ¸ˆã¿æ—¥ä»˜: {len(predicted_dates)} ä»¶")
        except Exception as e:
            print(f"[WARNING] äºˆæ¸¬æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

    total = len(df)
    predictor = None
    input_size = 0

    for i in range(10, total):
        test_row = df.iloc[i]
        test_date = test_row["æŠ½ã›ã‚“æ—¥"]
        test_date_str = test_date.strftime("%Y-%m-%d")

        # âœ… ã™ã§ã«äºˆæ¸¬æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        if test_date.date() in predicted_dates:
            print(f"[SKIP] æ—¢ã«äºˆæ¸¬æ¸ˆã¿: {test_date_str}")
            continue

        print(f"\n=== {test_date_str} ã®äºˆæ¸¬ã‚’é–‹å§‹ï¼ˆ{i}/{total - 1}ï¼‰ ===")
        latest_data = df.iloc[i - 10:i]
        train_data = df.iloc[:i]

        # âœ… 50ä»¶ã”ã¨ã«å†å­¦ç¿’ ã¾ãŸã¯ åˆå›
        if predictor is None or (i - 10) % 50 == 0:
            print(f"[INFO] ãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ä¸­...ï¼ˆindex={i}ï¼‰")
            try:
                X_tmp, _, _, _ = preprocess_data(train_data)
                if X_tmp is None or X_tmp.shape[1] == 0:
                    print(f"[WARNING] {test_date_str} ã®ç‰¹å¾´é‡ãŒä¸æ­£ã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue
                input_size = X_tmp.shape[1]
                predictor = LotoPredictor(input_size, 128, 7)
                success = predictor.train_model(train_data)
                if not success:
                    print(f"[ERROR] {test_date_str} ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue
            except Exception as e:
                print(f"[ERROR] {test_date_str} ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¾‹å¤–: {e}")
                traceback.print_exc()
                continue

        # äºˆæ¸¬
        try:
            predictions, confidence_scores = predictor.predict(latest_data)
            if predictions is None:
                print(f"[ERROR] {test_date_str} ã®äºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue
        except Exception as e:
            print(f"[ERROR] {test_date_str} ã®äºˆæ¸¬ä¸­ã«ä¾‹å¤–: {e}")
            traceback.print_exc()
            continue

        # æ¤œè¨¼ãƒ»ä¿å­˜
        try:
            verified_predictions = verify_predictions(list(zip(predictions, confidence_scores)), train_data)
            save_self_predictions(verified_predictions)
            save_predictions_to_csv(verified_predictions, test_date_str)
        except Exception as e:
            print(f"[ERROR] {test_date_str} ã®äºˆæ¸¬çµæœä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()

        # è©•ä¾¡
        try:
            print(f"[INFO] {test_date_str} ã®äºˆæ¸¬ç²¾åº¦ã‚’è©•ä¾¡ä¸­...")
            evaluate_prediction_accuracy_with_bonus("loto7_predictions.csv", "loto7.csv")
        except Exception as e:
            print(f"[ERROR] {test_date_str} è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()

    print(f"\n=== ä¸€æ‹¬äºˆæ¸¬ã¨è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸ [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ===")

if __name__ == "__main__":
    # main_with_improved_predictions()
    bulk_predict_all_past_draws()
