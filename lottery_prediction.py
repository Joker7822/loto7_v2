
# === Optunaã§æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ===
import os
import json

optuna_dir = "optuna_results"
optimized_params = {}

if os.path.exists(optuna_dir):
    for filename in os.listdir(optuna_dir):
        if filename.endswith(".json"):
            with open(os.path.join(optuna_dir, filename), "r") as f:
                optimized_params[filename.replace(".json", "")] = json.load(f)
    print(f"[INFO] Optunaæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©ç”¨ã—ã¾ã—ãŸ: {list(optimized_params.keys())}")
else:
    print("[INFO] æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å®Ÿè¡Œã—ã¾ã™ã€‚")


import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import stat
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor
from sklearn.ensemble import (
    GradientBoostingRegressor, RandomForestRegressor, StackingRegressor,
    GradientBoostingClassifier, RandomForestClassifier, HistGradientBoostingClassifier,
    AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier
)
from sklearn.linear_model import Ridge, LogisticRegression, RidgeClassifier, Perceptron, PassiveAggressiveClassifier, SGDClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.neural_network import MLPRegressor
from catboost import CatBoostRegressor
from lightgbm import LGBMRegressor
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.ensemble import StackingRegressor
from statsmodels.tsa.arima.model import ARIMA
from stable_baselines3 import PPO
from gymnasium import spaces
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import optuna
import matplotlib
matplotlib.use('Agg')  # â† â˜… ã“ã®è¡Œã‚’å…ˆã«è¿½åŠ ï¼
import matplotlib.pyplot as plt
import aiohttp
import asyncio
import warnings
import re
import platform
import gymnasium as gym
import sys
import os
import random
from sklearn.metrics import precision_score, recall_score, f1_score
from neuralforecast.models import TFT
from neuralforecast import NeuralForecast
import onnxruntime
import streamlit as st
from autogluon.tabular import TabularPredictor
import torch.backends.cudnn
from datetime import datetime 
from itertools import combinations
import shutil
import traceback
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
import tensorflow as tf
from gymnasium.utils import seeding
import time
import subprocess

# Windowsç’°å¢ƒã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãƒãƒªã‚·ãƒ¼ã‚’è¨­å®š
if platform.system() == "Windows":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

warnings.filterwarnings("ignore")


def _save_all_models_no_self(predictor, model_dir):
    os.makedirs(model_dir, exist_ok=True)
    try:
        def _save(path, save_fn):
            os.makedirs(os.path.dirname(path), exist_ok=True)
            save_fn(path)
            print(f"[INFO] ä¿å­˜å®Œäº†: {path}")

        # LSTM ONNX (if produced externally)
        if os.path.exists("lstm_model.onnx"):
            dst = os.path.join(model_dir, "lstm_model.onnx")
            shutil.copyfile("lstm_model.onnx", dst)
            print(f"[INFO] ä¿å­˜å®Œäº†: {dst}")

        # GAN
        try:
            g = getattr(predictor, "gan_model", None)
            if g is not None:
                _save(os.path.join(model_dir, "gan_model.pth"), lambda p: torch.save(g.state_dict(), p))
        except Exception as e:
            print("[WARN] GANä¿å­˜å¤±æ•—:", e)

        # PPO
        try:
            ppo = getattr(predictor, "ppo_model", None)
            if ppo is not None:
                out = os.path.join(model_dir, "ppo_model.zip")
                ppo.save(out)
                print(f"[INFO] ä¿å­˜å®Œäº†: {out}")
        except Exception as e:
            print("[WARN] PPOä¿å­˜å¤±æ•—:", e)

        # Diffusion
        try:
            dm = getattr(predictor, "diffusion_model", None)
            if dm is not None:
                _save(os.path.join(model_dir, "diffusion_model.pth"), lambda p: torch.save(dm.state_dict(), p))
        except Exception as e:
            print("[WARN] Diffusionä¿å­˜å¤±æ•—:", e)

        # GNN
        try:
            gnn = getattr(predictor, "gnn_model", None)
            if gnn is not None:
                _save(os.path.join(model_dir, "gnn_model.pth"), lambda p: torch.save(gnn.state_dict(), p))
        except Exception as e:
            print("[WARN] GNNä¿å­˜å¤±æ•—:", e)

        # TabNet
        try:
            tabnet = getattr(predictor, "tabnet_model", None)
            if tabnet is not None:
                from tabnet_module import save_tabnet_model
                save_tabnet_model(tabnet, os.path.join(model_dir, "tabnet_model"))
                print("[INFO] ä¿å­˜å®Œäº†: TabNet")
        except Exception as e:
            print("[WARN] TabNetä¿å­˜å¤±æ•—:", e)

        # BNN
        try:
            bnn = getattr(predictor, "bnn_model", None)
            bnn_guide = getattr(predictor, "bnn_guide", None)
            if bnn is not None and bnn_guide is not None:
                from bnn_module import save_bayesian_model
                save_bayesian_model(bnn, bnn_guide, os.path.join(model_dir, "bnn_model"))
                print("[INFO] ä¿å­˜å®Œäº†: BNN")
        except Exception as e:
            print("[WARN] BNNä¿å­˜å¤±æ•—:", e)

        # AutoGluon per position
        try:
            for j in range(7):
                model_path = f"autogluon_model_pos{j}"
                dest = os.path.join(model_dir, f"autogluon_model_pos{j}")
                if os.path.isdir(model_path):
                    if os.path.isdir(dest):
                        shutil.rmtree(dest)
                    shutil.copytree(model_path, dest)
            print("[INFO] ä¿å­˜å®Œäº†: AutoGluon")
        except Exception as e:
            print("[WARN] AutoGluonä¿å­˜å¤±æ•—:", e)

        print(f"[INFO] ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ â†’ {model_dir}")
        git_commit_and_push(model_dir, "Save trained models")
    except Exception as e:
        print(f"[WARNING] ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        traceback.print_exc()

def set_global_seed(seed=42):

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    # For deterministic behavior
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def git_commit_and_push(file_path, message):
    try:
        subprocess.run(["git", "add", file_path], check=True)
        diff = subprocess.run(["git", "diff", "--cached", "--quiet"])
        if diff.returncode != 0:
            subprocess.run(["git", "config", "--global", "user.name", "github-actions"], check=True)
            subprocess.run(["git", "config", "--global", "user.email", "github-actions@github.com"], check=True)
            subprocess.run(["git", "commit", "-m", message], check=True)
            subprocess.run(["git", "push"], check=True)
        else:
            print(f"[INFO] No changes in {file_path}")
    except Exception as e:
        print(f"[WARNING] Git commit/push failed: {e}")

def get_valid_num_heads(embed_dim, max_heads=8):
    for h in reversed(range(1, max_heads + 1)):
        if embed_dim % h == 0:
            return h
    return 1

def add_noise_to_features(X, noise_level=0.02):
    import numpy as np
    noise = np.random.normal(loc=0.0, scale=noise_level, size=X.shape)
    return X + noise
    
class LotoGAN(nn.Module):
    def __init__(self, noise_dim=100):
        super(LotoGAN, self).__init__()
        self.generator = nn.Sequential(
            nn.Linear(noise_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 37),
            nn.Sigmoid()
        )
        self.discriminator = nn.Sequential(
            nn.Linear(37, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
        self.noise_dim = noise_dim

    def generate_samples(self, num_samples):
        noise = torch.randn(num_samples, self.noise_dim)
        with torch.no_grad():
            samples = self.generator(noise)
        return samples.numpy()

def create_advanced_features(dataframe):

    def convert_to_number_list(x):
        if isinstance(x, str):
            cleaned = x.strip("[]").replace(",", " ").replace("'", "").replace('"', "")
            return [int(n) for n in cleaned.split() if n.isdigit()]
        return x if isinstance(x, list) else [0]

    dataframe['æœ¬æ•°å­—'] = dataframe['æœ¬æ•°å­—'].apply(convert_to_number_list)
    dataframe['ãƒœãƒ¼ãƒŠã‚¹æ•°å­—'] = dataframe['ãƒœãƒ¼ãƒŠã‚¹æ•°å­—'].apply(convert_to_number_list)
    dataframe['æŠ½ã›ã‚“æ—¥'] = pd.to_datetime(dataframe['æŠ½ã›ã‚“æ—¥'])

    valid_mask = (dataframe['æœ¬æ•°å­—'].apply(len) == 7) & (dataframe['ãƒœãƒ¼ãƒŠã‚¹æ•°å­—'].apply(len) == 2)
    dataframe = dataframe[valid_mask].copy()

    nums_array = np.vstack(dataframe['æœ¬æ•°å­—'].values)
    sorted_nums = np.sort(nums_array, axis=1)
    diffs = np.diff(sorted_nums, axis=1)

    features = pd.DataFrame(index=dataframe.index)
    features['å¥‡æ•°æ¯”'] = (nums_array % 2 != 0).sum(axis=1) / nums_array.shape[1]
    features['æœ¬æ•°å­—åˆè¨ˆ'] = nums_array.sum(axis=1)
    features['ãƒ¬ãƒ³ã‚¸'] = nums_array.max(axis=1) - nums_array.min(axis=1)
    features['æ¨™æº–åå·®'] = np.std(nums_array, axis=1)
    features['æ›œæ—¥'] = dataframe['æŠ½ã›ã‚“æ—¥'].dt.dayofweek
    features['æœˆ'] = dataframe['æŠ½ã›ã‚“æ—¥'].dt.month
    features['å¹´'] = dataframe['æŠ½ã›ã‚“æ—¥'].dt.year
    features['é€£ç•ªæ•°'] = (diffs == 1).sum(axis=1)
    features['æœ€å°é–“éš”'] = diffs.min(axis=1)
    features['æœ€å¤§é–“éš”'] = diffs.max(axis=1)
    features['æ•°å­—å¹³å‡'] = nums_array.mean(axis=1)
    features['å¶æ•°æ¯”'] = (nums_array % 2 == 0).sum(axis=1) / nums_array.shape[1]
    features['ä¸­å¤®å€¤'] = np.median(nums_array, axis=1)

    # å‡ºç¾é–“éš”å¹³å‡ï¼ˆãƒ«ãƒ¼ãƒ—ï¼‰
    last_seen = {}
    gaps = []
    for idx, nums in dataframe['æœ¬æ•°å­—'].items():
        gap = [idx - last_seen.get(n, idx) for n in nums]
        gaps.append(np.mean(gap))
        for n in nums:
            last_seen[n] = idx
    features['å‡ºç¾é–“éš”å¹³å‡'] = gaps

    # å‡ºç¾é »åº¦ã‚¹ã‚³ã‚¢ï¼ˆ1å›ã®ã¿ç®—å‡ºã—ä½¿ã„å›ã™ï¼‰
    all_numbers = [num for nums in dataframe['æœ¬æ•°å­—'] for num in nums]
    all_freq = pd.Series(all_numbers).value_counts().to_dict()
    features['å‡ºç¾é »åº¦ã‚¹ã‚³ã‚¢'] = dataframe['æœ¬æ•°å­—'].apply(lambda nums: sum(all_freq.get(n, 0) for n in nums) / len(nums))

    # ãƒšã‚¢ãƒ»ãƒˆãƒªãƒ—ãƒ«é »åº¦ã‚‚å…ˆã«ã¾ã¨ã‚ã¦æ§‹ç¯‰ï¼ˆé‡è¤‡å‡¦ç†ã‚’æ’é™¤ï¼‰
    pair_freq, triple_freq = {}, {}
    for nums in dataframe['æœ¬æ•°å­—']:
        for pair in combinations(sorted(nums), 2):
            pair_freq[pair] = pair_freq.get(pair, 0) + 1
        for triple in combinations(sorted(nums), 3):
            triple_freq[triple] = triple_freq.get(triple, 0) + 1

    features['ãƒšã‚¢å‡ºç¾é »åº¦'] = dataframe['æœ¬æ•°å­—'].apply(
        lambda nums: sum(pair_freq.get(tuple(sorted((nums[i], nums[j]))), 0) for i in range(len(nums)) for j in range(i+1, len(nums)))
    )
    features['ãƒˆãƒªãƒ—ãƒ«å‡ºç¾é »åº¦'] = dataframe['æœ¬æ•°å­—'].apply(
        lambda nums: sum(triple_freq.get(tuple(sorted((nums[i], nums[j], nums[k]))), 0) for i in range(len(nums)) for j in range(i+1, len(nums)) for k in range(j+1, len(nums)))
    )

    # ç›´è¿‘5å›å‡ºç¾ç‡
    past_5_counts = {}
    for i, row in dataframe.iterrows():
        nums = row['æœ¬æ•°å­—']
        recent = dataframe[dataframe['æŠ½ã›ã‚“æ—¥'] < row['æŠ½ã›ã‚“æ—¥']].tail(5)
        recent_nums = [num for nums in recent['æœ¬æ•°å­—'] for num in nums]
        count = sum(n in recent_nums for n in nums)
        past_5_counts[i] = count / len(nums)
    features['ç›´è¿‘5å›å‡ºç¾ç‡'] = features.index.map(past_5_counts)

    return pd.concat([dataframe, features], axis=1)

def preprocess_data(data):
    """ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†: ç‰¹å¾´é‡ã®ä½œæˆ & ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°"""
    
    # ç‰¹å¾´é‡ä½œæˆ
    processed_data = create_advanced_features(data)

    if processed_data.empty:
        print("ã‚¨ãƒ©ãƒ¼: ç‰¹å¾´é‡ç”Ÿæˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None, None, None

    print("=== ç‰¹å¾´é‡ä½œæˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ ===")
    print(processed_data.head())

    # æ•°å€¤ç‰¹å¾´é‡ã®é¸æŠ
    numeric_features = processed_data.select_dtypes(include=[np.number]).columns
    X = processed_data[numeric_features].fillna(0)  # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹

    print(f"æ•°å€¤ç‰¹å¾´é‡ã®æ•°: {len(numeric_features)}, ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}")

    if X.empty:
        print("ã‚¨ãƒ©ãƒ¼: æ•°å€¤ç‰¹å¾´é‡ãŒä½œæˆã•ã‚Œãšã€ãƒ‡ãƒ¼ã‚¿ãŒç©ºã«ãªã£ã¦ã„ã¾ã™ã€‚")
        return None, None, None

    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    print("=== ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œã®ãƒ‡ãƒ¼ã‚¿ ===")
    print(X_scaled[:5])  # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º

    # ç›®æ¨™å¤‰æ•°ã®æº–å‚™
    try:
        y = np.array([list(map(int, nums)) for nums in processed_data['æœ¬æ•°å­—']])
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: ç›®æ¨™å¤‰æ•°ã®ä½œæˆæ™‚ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None, None, None

    return X_scaled, y, scaler

def convert_numbers_to_binary_vectors(data):
    """
    æœ¬æ•°å­—ã‚’0/1ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã™ã‚‹
    ä¾‹ï¼š[1,5,7,22,28,30,36] â†’ [1,0,0,0,1,0,1, ..., 0,1]
    """
    vectors = []
    for numbers in data['æœ¬æ•°å­—']:
        vec = np.zeros(37)
        for n in numbers:
            if 1 <= n <= 37:
                vec[n-1] = 1
        vectors.append(vec)
    return np.array(vectors)

def calculate_prediction_errors(predictions, actual_numbers):
    """äºˆæ¸¬å€¤ã¨å®Ÿéš›ã®å½“é¸çµæœã®èª¤å·®ã‚’è¨ˆç®—ã—ã€ç‰¹å¾´é‡ã¨ã—ã¦ä¿å­˜"""
    errors = []
    for pred, actual in zip(predictions, actual_numbers):
        pred_numbers = set(pred[0])
        actual_numbers = set(actual)
        error_count = len(actual_numbers - pred_numbers)
        errors.append(error_count)
    
    return np.mean(errors)

def save_self_predictions(predictions, file_path="self_predictions.csv", max_records=100):
    """äºˆæ¸¬çµæœã‚’CSVã«ä¿å­˜ã—ã€ä¿å­˜ä»¶æ•°ã‚’æœ€å¤§max_recordsã«åˆ¶é™ã—ã€ä¸–ä»£ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä¿å­˜"""
    rows = []
    for numbers, confidence in predictions:
        rows.append(numbers.tolist())

    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    if os.path.exists(file_path):
        existing = pd.read_csv(file_path, header=None).values.tolist()
        rows = existing + rows

    # æœ€æ–°max_recordsä»¶ã ã‘æ®‹ã™
    rows = rows[-max_records:]

    df = pd.DataFrame(rows)

    # --- ãƒ¡ã‚¤ãƒ³ä¿å­˜ ---
    df.to_csv(file_path, index=False, header=False)
    print(f"[INFO] è‡ªå·±äºˆæ¸¬çµæœã‚’ {file_path} ã«ä¿å­˜ã—ã¾ã—ãŸï¼ˆæœ€å¤§{max_records}ä»¶ï¼‰")

    # --- ğŸ”¥ ä¸–ä»£ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚‚å°‚ç”¨ãƒ•ã‚©ãƒ«ãƒ€ã«å¤‰æ›´ ---
    gen_dir = "self_predictions_gen"
    os.makedirs(gen_dir, exist_ok=True)  # ãƒ•ã‚©ãƒ«ãƒ€ãŒãªã‘ã‚Œã°ä½œæˆ

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    generation_file = os.path.join(gen_dir, f"self_predictions_gen_{timestamp}.csv")
    df.to_csv(generation_file, index=False, header=False)
    print(f"[INFO] ä¸–ä»£åˆ¥ã«è‡ªå·±äºˆæ¸¬ã‚‚ä¿å­˜ã—ã¾ã—ãŸ: {generation_file}")

def load_self_predictions(file_path="self_predictions.csv", min_match_threshold=3, true_data=None):
    if not os.path.exists(file_path):
        print(f"[INFO] è‡ªå·±äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    try:
        # ğŸ”¥ é«˜é€Ÿç‰ˆã«ç½®ãæ›ãˆï¼
        df = pd.read_csv(file_path, header=None)
        df = df.dropna()
        df = df[df.apply(lambda row: all(1 <= x <= 37 for x in row), axis=1)]
        numbers_list = df.values.tolist()

        if true_data is not None:
            scores = evaluate_self_predictions(numbers_list, true_data)
            filtered_rows = [r for r, s in zip(numbers_list, scores) if s >= min_match_threshold]
            print(f"[INFO] ä¸€è‡´æ•°{min_match_threshold}ä»¥ä¸Šã®è‡ªå·±äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿: {len(filtered_rows)}ä»¶")
            return filtered_rows
        else:
            return numbers_list

    except Exception as e:
        print(f"[ERROR] è‡ªå·±äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def evaluate_self_predictions(self_predictions, true_data):
    """
    è‡ªå·±äºˆæ¸¬ãƒªã‚¹ãƒˆã¨æœ¬ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒã—ã¦ä¸€è‡´æ•°ã‚’è©•ä¾¡
    :param self_predictions: [[5,12,17,22,30,34,37], ...]
    :param true_data: éå»ã®æœ¬ç‰©æœ¬æ•°å­—ãƒ‡ãƒ¼ã‚¿ï¼ˆdata['æœ¬æ•°å­—'].tolist()ï¼‰
    :return: å„è‡ªå·±äºˆæ¸¬ã«å¯¾å¿œã™ã‚‹æœ€å¤§ä¸€è‡´æ•°ãƒªã‚¹ãƒˆ
    """
    scores = []
    true_sets = [set(nums) for nums in true_data]

    for pred in self_predictions:
        pred_set = set(pred)
        max_match = 0
        for true_set in true_sets:
            match = len(pred_set & true_set)
            if match > max_match:
                max_match = match
        scores.append(max_match)

    return scores

def update_features_based_on_results(data, accuracy_results):
    """éå»ã®äºˆæ¸¬çµæœã¨å®Ÿéš›ã®çµæœã®æ¯”è¼ƒã‹ã‚‰ç‰¹å¾´é‡ã‚’æ›´æ–°"""
    
    for result in accuracy_results:
        event_date = result["æŠ½ã›ã‚“æ—¥"]
        max_matches = result["æœ€é«˜ä¸€è‡´æ•°"]
        avg_matches = result["å¹³å‡ä¸€è‡´æ•°"]
        confidence_avg = result["ä¿¡é ¼åº¦å¹³å‡"]

        # éå»ã®ãƒ‡ãƒ¼ã‚¿ã«äºˆæ¸¬ç²¾åº¦ã‚’çµ„ã¿è¾¼ã‚€
        data.loc[data["æŠ½ã›ã‚“æ—¥"] == event_date, "éå»ã®æœ€å¤§ä¸€è‡´æ•°"] = max_matches
        data.loc[data["æŠ½ã›ã‚“æ—¥"] == event_date, "éå»ã®å¹³å‡ä¸€è‡´æ•°"] = avg_matches
        data.loc[data["æŠ½ã›ã‚“æ—¥"] == event_date, "éå»ã®äºˆæ¸¬ä¿¡é ¼åº¦"] = confidence_avg

    # ç‰¹å¾´é‡ãŒãªã„å ´åˆã¯0ã§åŸ‹ã‚ã‚‹
    data["éå»ã®æœ€å¤§ä¸€è‡´æ•°"] = data["éå»ã®æœ€å¤§ä¸€è‡´æ•°"].fillna(0)
    data["éå»ã®å¹³å‡ä¸€è‡´æ•°"] = data["éå»ã®å¹³å‡ä¸€è‡´æ•°"].fillna(0)
    data["éå»ã®äºˆæ¸¬ä¿¡é ¼åº¦"] = data["éå»ã®äºˆæ¸¬ä¿¡é ¼åº¦"].fillna(0)

    return data

class LotoLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LotoLSTM, self).__init__()
        self.lstm = nn.GRU(input_size, hidden_size, batch_first=True, bidirectional=True)
        self.attn = nn.Linear(hidden_size * 2, 1)
        self.fc = nn.Linear(hidden_size * 2, output_size)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)  # [batch, seq_len, hidden*2]
        attn_weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)  # [batch, seq_len]
        context = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # [batch, hidden*2]
        out = self.fc(context)
        return out

def train_lstm_model(X_train, y_train, input_size, device):
    
    torch.backends.cudnn.benchmark = True  # â˜…ã“ã‚Œã‚’è¿½åŠ 
    
    model = LotoLSTM(input_size=input_size, hidden_size=128, output_size=7).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    dataset = TensorDataset(
        torch.tensor(X_train, dtype=torch.float32),
        torch.tensor(y_train, dtype=torch.float32)
    )
    loader = DataLoader(dataset, batch_size=32, shuffle=True, pin_memory=True, num_workers=2)  # â˜…å¤‰æ›´

    scaler = torch.cuda.amp.GradScaler()  # â˜…Mixed Precisionè¿½åŠ 

    model.train()
    for epoch in range(50):
        total_loss = 0
        for batch_X, batch_y in loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            optimizer.zero_grad()
            with torch.cuda.amp.autocast():  # â˜…ã“ã“ã‚‚Mixed Precision
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            total_loss += loss.item()
        print(f"[LSTM] Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}")

    # ONNXã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    dummy_input = torch.randn(1, 1, input_size).to(device)
    torch.onnx.export(
        model,
        dummy_input,
        "lstm_model.onnx",
        input_names=["input"],
        output_names=["output"],
        dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
        opset_version=12
    )
    print("[INFO] LSTM ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†")
    return model

def extract_high_accuracy_combinations(evaluation_df, threshold=5):
    high_matches = evaluation_df[evaluation_df["æœ¬æ•°å­—ä¸€è‡´æ•°"] >= threshold]
    return high_matches

def convert_hit_combos_to_training_data(hit_combos, original_data):
    temp_df = original_data.copy()
    new_rows = []
    for _, row in hit_combos.iterrows():
        temp = {
            "æŠ½ã›ã‚“æ—¥": row["æŠ½ã›ã‚“æ—¥"],
            "æœ¬æ•°å­—": row["äºˆæ¸¬ç•ªå·"],
            "ãƒœãƒ¼ãƒŠã‚¹æ•°å­—": row["å½“é¸ãƒœãƒ¼ãƒŠã‚¹"]
        }
        new_rows.append(temp)
    if not new_rows:
        return None, None
    temp_df = pd.DataFrame(new_rows)
    return preprocess_data(temp_df)[:2]

# === ğŸ”§ Set Transformer ãƒ¢ãƒ‡ãƒ« ===
class SAB(nn.Module):
    def __init__(self, dim_in, dim_out, num_heads):
        super().__init__()
        self.mha = nn.MultiheadAttention(embed_dim=dim_in, num_heads=num_heads, batch_first=True)
        self.ln1 = nn.LayerNorm(dim_in)
        self.ff = nn.Sequential(
            nn.Linear(dim_in, dim_out),
            nn.ReLU(),
            nn.Linear(dim_out, dim_out)
        )
        self.ln2 = nn.LayerNorm(dim_out)

    def forward(self, x):
        h, _ = self.mha(x, x, x)
        x = self.ln1(x + h)
        h = self.ff(x)
        return self.ln2(x + h)

class SetTransformerRegressor(nn.Module):
    def __init__(self, input_dim, output_dim, num_sabs=2):
        super().__init__()
        num_heads = get_valid_num_heads(input_dim)  # â† å‹•çš„ã«æ±ºå®šï¼
        self.encoders = nn.ModuleList([
            SAB(input_dim, input_dim, num_heads) for _ in range(num_sabs)
        ])
        self.fc = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        for sab in self.encoders:
            x = sab(x)
        x = x.mean(dim=1)
        return self.fc(x)

def train_set_transformer_model(X, y, input_dim, output_dim=7, epochs=30):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SetTransformerRegressor(input_dim, output_dim).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()

    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))
    loader = DataLoader(dataset, batch_size=32, shuffle=True)

    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_X, batch_y in loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            batch_X = batch_X.unsqueeze(1)  # [batch, set_size=1, input_dim]
            optimizer.zero_grad()
            out = model(batch_X)
            loss = criterion(out, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"[SetTransformer] Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}")

    return model

def predict_with_set_transformer(model, X):
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    with torch.no_grad():
        inputs = torch.tensor(X, dtype=torch.float32).to(device).unsqueeze(1)
        outputs = model(inputs).cpu().numpy()
    return outputs

class LotoPredictor:
    def __init__(self, input_size, hidden_size, output_size):
        print("[INFO] ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–")
        self.lstm_model = LotoLSTM(input_size, hidden_size, output_size)
        self.regression_models = [None] * 7
        self.scaler = None
        self.onnx_session = None
        self.gan_model = None
        self.ppo_model = None
        self.feature_names = None  # AutoGluonç”¨ã«ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡å
        self.set_transformer_model = None
        self.diffusion_model = None
        self.diffusion_betas = None
        self.diffusion_alphas_cumprod = None
        self.regression_models = [None] * 7
        
        # --- GANãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰ ---
        if os.path.exists("gan_model.pth"):
            self.gan_model = LotoGAN()
            self.gan_model.load_state_dict(torch.load("gan_model.pth"))
            self.gan_model.eval()
            print("[INFO] GANãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

        # --- PPOãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰ ---
        if os.path.exists("ppo_model.zip"):
            self.ppo_model = PPO.load("ppo_model")
            print("[INFO] PPOãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

    def load_onnx_model(self, onnx_path="lstm_model.onnx"):
        print("[INFO] ONNX ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™")
        self.onnx_session = onnxruntime.InferenceSession(
            onnx_path,
            providers=['CPUExecutionProvider']
        )

    def predict_with_onnx(self, X):
        if self.onnx_session is None:
            print("[ERROR] ONNX ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None

        input_name = self.onnx_session.get_inputs()[0].name
        output = self.onnx_session.run(None, {input_name: X.astype(np.float32)})
        return output[0]

    def train_model(self, data, accuracy_results=None, model_dir="models/tmp"):
        os.makedirs(model_dir, exist_ok=True)
        set_global_seed(42)
        print("[INFO] ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’é–‹å§‹")

        data["æŠ½ã›ã‚“æ—¥"] = pd.to_datetime(data["æŠ½ã›ã‚“æ—¥"], errors='coerce')
        latest_valid_date = data["æŠ½ã›ã‚“æ—¥"].max()
        data = data[data["æŠ½ã›ã‚“æ—¥"] <= latest_valid_date]
        print(f"[INFO] æœªæ¥ãƒ‡ãƒ¼ã‚¿é™¤å¤–æ¸ˆ: {latest_valid_date.date()} ä»¥å‰ {len(data)}ä»¶")

        true_numbers = data['æœ¬æ•°å­—'].tolist()
        self_data = load_self_predictions(file_path="self_predictions.csv", min_match_threshold=6, true_data=true_numbers)
        high_match_combos = extract_high_match_patterns(data, min_match=6)

        if self_data or high_match_combos:
            print("[INFO] éå»ã®é«˜ä¸€è‡´è‡ªå·±äºˆæ¸¬ï¼‹é«˜ä¸€è‡´æœ¬ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã™")
            new_rows = []
            for nums in (self_data or []):
                new_rows.append({
                    'æŠ½ã›ã‚“æ—¥': pd.Timestamp.now(),
                    'å›å·': 9999,
                    'æœ¬æ•°å­—': nums,
                    'ãƒœãƒ¼ãƒŠã‚¹æ•°å­—': [0, 0]
                })
            for nums in (high_match_combos or []):
                new_rows.append({
                    'æŠ½ã›ã‚“æ—¥': pd.Timestamp.now(),
                    'å›å·': 9999,
                    'æœ¬æ•°å­—': nums,
                    'ãƒœãƒ¼ãƒŠã‚¹æ•°å­—': [0, 0]
                })
            if new_rows:
                new_data = pd.DataFrame(new_rows)
                data = pd.concat([data, new_data], ignore_index=True)

        X, y, self.scaler = preprocess_data(data)
        if X is None or y is None:
            print("[ERROR] å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return

        self.feature_names = [str(i) for i in range(X.shape[1])]
        processed_df = create_advanced_features(data)
        important_features = extract_strong_features(accuracy_results, processed_df) if accuracy_results else []
        print(f"[INFO] å¼·èª¿å¯¾è±¡ã®ç‰¹å¾´é‡: {important_features}")
        X = reinforce_features(X, self.feature_names, important_features, multiplier=1.5)

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        input_size = X_train.shape[1]
        print("[INFO] LSTM ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´é–‹å§‹")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        X_train_tensor = torch.tensor(X_train.reshape(-1, 1, input_size), dtype=torch.float32).to(device)
        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)
        self.lstm_model = train_lstm_model(X_train_tensor, y_train_tensor, input_size, device=device)

        dummy_input = torch.randn(1, 1, input_size)
        lstm_path = os.path.join(model_dir, "lstm_model.onnx")
        torch.onnx.export(self.lstm_model, dummy_input, lstm_path,
                        input_names=["input"], output_names=["output"],
                        dynamic_axes={"input": {0: "batch_size"}, "output": {0: "batch_size"}},
                        opset_version=12)
        self.load_onnx_model(lstm_path)

        print("[INFO] Set Transformer ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹")
        self.set_transformer_model = train_set_transformer_model(X_train, y_train, input_size)

        print("[INFO] Diffusionãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­")
        from diffusion_module import train_diffusion_ddpm
        real_data_bin = convert_numbers_to_binary_vectors(data)
        self.diffusion_model, self.diffusion_betas, self.diffusion_alphas_cumprod = train_diffusion_ddpm(real_data_bin)
        torch.save(self.diffusion_model.state_dict(), os.path.join(model_dir, "diffusion_model.pth"))

        print("[INFO] GNNãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­")
        from gnn_core import LotoGNN, build_cooccurrence_graph
        graph_data = build_cooccurrence_graph(data)
        self.gnn_model = LotoGNN()
        optimizer = torch.optim.Adam(self.gnn_model.parameters(), lr=0.01)
        for epoch in range(200):
            self.gnn_model.train()
            out = self.gnn_model(graph_data.x, graph_data.edge_index)
            loss = torch.nn.functional.mse_loss(out, graph_data.x)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            if epoch % 50 == 0:
                print(f"[GNN] Epoch {epoch} Loss: {loss.item():.4f}")
        torch.save(self.gnn_model.state_dict(), os.path.join(model_dir, "gnn_model.pth"))

        print("[INFO] TabNet ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­")
        from tabnet_module import train_tabnet, save_tabnet_model
        self.tabnet_model = train_tabnet(X_train, y_train)
        save_tabnet_model(self.tabnet_model, os.path.join(model_dir, "tabnet_model"))

        print("[INFO] BNN ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­")
        from bnn_module import train_bayesian_regression, save_bayesian_model
        self.bnn_model, self.bnn_guide = train_bayesian_regression(X_train, y_train, input_size)
        save_bayesian_model(self.bnn_model, self.bnn_guide, os.path.join(model_dir, "bnn_model"))

        print("[INFO] AutoGluon ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­")
        for i in range(7):
            df_train = pd.DataFrame(X_train)
            df_train['target'] = y_train[:, i]
            ag_path = os.path.join(model_dir, f"autogluon_model_pos{i}")
            predictor = TabularPredictor(label='target', path=ag_path, verbosity=0).fit(
                df_train,
                excluded_model_types=['KNN', 'NN_TORCH'],
                hyperparameters={
                    'GBM': {'device': 'CPU', 'num_boost_round': 300},
                    'XGB': {'tree_method': 'hist', 'n_estimators': 300},
                    'CAT': {'task_type': 'CPU', 'iterations': 300},
                    'RF': {'n_estimators': 200}
                },
                num_gpus=1,
                ag_args_fit={'random_seed': 42}
            )
            self.regression_models[i] = predictor
            print(f"[DEBUG] AutoGluon ãƒ¢ãƒ‡ãƒ« {i+1}/7 å®Œäº†")

        print("[INFO] GAN ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­")
        real_data_tensor = torch.tensor(real_data_bin, dtype=torch.float32)
        gan = LotoGAN()
        optimizer_G = optim.Adam(gan.generator.parameters(), lr=0.001)
        optimizer_D = optim.Adam(gan.discriminator.parameters(), lr=0.001)
        criterion = nn.BCELoss()
        for epoch in range(3000):
            set_global_seed(10000 + epoch)
            idx = np.random.randint(0, real_data_tensor.size(0), 32)
            real_batch = real_data_tensor[idx]
            noise = torch.randn(32, gan.noise_dim)
            fake_batch = gan.generator(noise)
            real_labels = torch.ones(32, 1)
            fake_labels = torch.zeros(32, 1)

            optimizer_D.zero_grad()
            d_loss = (criterion(gan.discriminator(real_batch), real_labels) +
                    criterion(gan.discriminator(fake_batch), fake_labels)) / 2
            d_loss.backward()
            optimizer_D.step()

            optimizer_G.zero_grad()
            fake_batch = gan.generator(torch.randn(32, gan.noise_dim))
            g_loss = criterion(gan.discriminator(fake_batch), real_labels)
            g_loss.backward()
            optimizer_G.step()

            if (epoch + 1) % 500 == 0:
                print(f"[GAN] Epoch {epoch+1} D: {d_loss.item():.4f} G: {g_loss.item():.4f}")
        self.gan_model = gan
        torch.save(self.gan_model.state_dict(), os.path.join(model_dir, "gan_model.pth"))

        print("[INFO] PPO ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ä¸­")
        from stable_baselines3 import PPO
        from stable_baselines3.common.vec_env import DummyVecEnv
        from reinforcement_env import LotoEnv  # ã‚ãªãŸã® LotoEnv ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã«åˆã‚ã›ã¦ãã ã•ã„

        historical_numbers = [n for nums in data['æœ¬æ•°å­—'].tolist() for n in nums]
        env = DummyVecEnv([lambda: LotoEnv(historical_numbers)])

        self.ppo_model = PPO("MlpPolicy", env, seed=42, verbose=0)
        self.ppo_model.learn(total_timesteps=50000)
        self.ppo_model.save(os.path.join(model_dir, "ppo_model.zip"))

        print("[INFO] å…¨ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸ")

    def load_saved_models(self, model_dir):

        # LSTM
        onnx_path = os.path.join(model_dir, "lstm_model.onnx")
        if os.path.exists(onnx_path):
            self.load_onnx_model(onnx_path)
            print("[INFO] LSTM (ONNX) ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

        # GAN
        gan_path = os.path.join(model_dir, "gan_model.pth")
        if os.path.exists(gan_path):
            from gnn_core import LotoGAN
            self.gan_model = LotoGAN()
            self.gan_model.load_state_dict(torch.load(gan_path))
            self.gan_model.eval()
            print("[INFO] GANãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

        # PPO
        ppo_path = os.path.join(model_dir, "ppo_model.zip")
        if os.path.exists(ppo_path):
            self.ppo_model = PPO.load(ppo_path)
            print("[INFO] PPOãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

        # Diffusion
        diff_path = os.path.join(model_dir, "diffusion_model.pth")
        if os.path.exists(diff_path):
            from diffusion_module import DiffusionModel, get_diffusion_constants
            self.diffusion_model = DiffusionModel()
            self.diffusion_model.load_state_dict(torch.load(diff_path))
            self.diffusion_model.eval()
            self.diffusion_betas, self.diffusion_alphas_cumprod = get_diffusion_constants()
            print("[INFO] Diffusion ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

        # GNN
        gnn_path = os.path.join(model_dir, "gnn_model.pth")
        if os.path.exists(gnn_path):
            from gnn_core import LotoGNN
            self.gnn_model = LotoGNN()
            self.gnn_model.load_state_dict(torch.load(gnn_path))
            self.gnn_model.eval()
            print("[INFO] GNN ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

        # TabNet
        tabnet_path = os.path.join(model_dir, "tabnet_model")
        if os.path.exists(tabnet_path):
            from tabnet_module import load_tabnet_model
            self.tabnet_model = load_tabnet_model(tabnet_path)
            print("[INFO] TabNet ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

        # BNN
        bnn_path = os.path.join(model_dir, "bnn_model")
        if os.path.exists(bnn_path):
            from bnn_module import load_bayesian_model
            self.bnn_model, self.bnn_guide = load_bayesian_model(bnn_path)
            print("[INFO] BNN ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

        # AutoGluon
        for j in range(7):
            ag_path = os.path.join(model_dir, f"autogluon_model_pos{j}")
            if os.path.exists(ag_path):
                from autogluon.tabular import TabularPredictor
                self.regression_models[j] = TabularPredictor.load(ag_path)
                print(f"[INFO] AutoGluon ãƒ¢ãƒ‡ãƒ« {j} ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

    def predict(self, latest_data, num_candidates=50):
        print(f"[INFO] äºˆæ¸¬ã‚’é–‹å§‹ï¼ˆå€™è£œæ•°: {num_candidates}ï¼‰")
        X, _, _ = preprocess_data(latest_data)

        if X is None or len(X) == 0:
            print("[ERROR] äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            return None, None

        print(f"[DEBUG] äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿ã® shape: {X.shape}")

        freq_score = calculate_number_frequencies(latest_data)
        cycle_score = calculate_number_cycle_score(latest_data)
        all_predictions = []

        def append_prediction(numbers, base_confidence=0.8):
            numbers = [int(n) for n in numbers]  # â† å®‰å…¨ã‚­ãƒ£ã‚¹ãƒˆ
            score = sum(freq_score.get(n, 0) for n in numbers) - sum(cycle_score.get(n, 0) for n in numbers)
            confidence = base_confidence + (score / 500.0)
            all_predictions.append((numbers, confidence))

        try:
            X_df = pd.DataFrame(X)

            if self.feature_names:
                for name in self.feature_names:
                    if name not in X_df.columns:
                        X_df[name] = 0.0
                X_df = X_df[self.feature_names]
                X = X_df.values
            else:
                print("[WARNING] self.feature_names ãŒæœªå®šç¾©ã§ã™")

            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

            for i in range(num_candidates):
                set_global_seed(100 + i)

                ml_predictions = np.array([
                    self.regression_models[j].predict(X_df) for j in range(7)
                ]).T

                self.lstm_model.to(device)
                self.lstm_model.eval()
                X_tensor = torch.tensor(X.reshape(-1, 1, X.shape[1]), dtype=torch.float32).to(device)
                with torch.no_grad():
                    lstm_predictions = self.lstm_model(X_tensor).detach().cpu().numpy()

                final_predictions = (ml_predictions + lstm_predictions) / 2

                if self.set_transformer_model:
                    st_predictions = predict_with_set_transformer(self.set_transformer_model, X)
                    final_predictions = (final_predictions + st_predictions) / 2

                if hasattr(self, "tabnet_model") and self.tabnet_model is not None:
                    from tabnet_module import predict_tabnet
                    tabnet_preds = predict_tabnet(self.tabnet_model, X)
                    final_predictions = (final_predictions + tabnet_preds) / 2

                for pred in final_predictions:
                    numbers = np.round(pred).astype(int)
                    numbers = np.clip(numbers, 1, 37)
                    numbers = np.sort(numbers)
                    append_prediction(numbers, base_confidence=1.0)

            if self.gan_model:
                for i in range(num_candidates):
                    set_global_seed(int(time.time() * 1000) % 100000 + i)  # æ¯å›ç•°ãªã‚‹ã‚·ãƒ¼ãƒ‰
                    gan_sample = self.gan_model.generate_samples(1)[0]
                
                    # â˜… æ•°å­—ã«ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’è¿½åŠ ï¼ˆä¾‹ï¼šæ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰
                    logits = gan_sample / 0.7  # "æ¸©åº¦" ã‚’ä¸‹ã’ã‚‹ã¨ã‚·ãƒ£ãƒ¼ãƒ—ã«ã€é«˜ãã™ã‚‹ã¨å¤šæ§˜ã«
                    probs = logits / logits.sum()
                    numbers = np.random.choice(37, 7, replace=False, p=probs)
                    
                    append_prediction(np.sort(numbers + 1), base_confidence=0.8)

            if self.ppo_model:
                for i in range(num_candidates):
                    set_global_seed(random.randint(1000, 999999))  # ğŸ” ã‚·ãƒ¼ãƒ‰ã‚’æ¯å›å¤‰æ›´
                    obs = np.zeros(37, dtype=np.float32)
                
                    # å¤šæ§˜æ€§ç¢ºä¿ã®ãŸã‚ deterministic=False ã«å¤‰æ›´
                    action, _ = self.ppo_model.predict(obs, deterministic=False)
                
                    numbers = np.argsort(action)[-7:] + 1
                    append_prediction(np.sort(numbers), base_confidence=0.85)

            if self.diffusion_model:
                from diffusion_module import sample_diffusion_ddpm
                print("[INFO] Diffusion ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç”Ÿæˆã‚’é–‹å§‹")
            
                for i in range(num_candidates):
                    set_global_seed(random.randint(1000, 999999))  # ğŸ” ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’æ¯å›å¤‰ãˆã‚‹
            
                    try:
                        sample = sample_diffusion_ddpm(
                            self.diffusion_model,
                            self.diffusion_betas,
                            self.diffusion_alphas_cumprod,
                            dim=37,
                            num_samples=1  # â˜… 1ä»¶ãšã¤ç”Ÿæˆã—ã¦å¤šæ§˜æ€§ã‚’ç¢ºä¿
                        )[0]
            
                        numbers = np.argsort(sample)[-7:] + 1
                        numbers = np.sort(numbers)
                        append_prediction(numbers, base_confidence=0.84)
            
                    except Exception as e:
                        print(f"[WARNING] Diffusion ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

            if self.gnn_model:
                from gnn_core import build_cooccurrence_graph
                print("[INFO] GNNæ¨è«–ã‚’é–‹å§‹")
                graph_data = build_cooccurrence_graph(latest_data)
                self.gnn_model.eval()
                with torch.no_grad():
                    gnn_scores = self.gnn_model(graph_data.x, graph_data.edge_index).squeeze().numpy()
                    for i in range(num_candidates):
                        set_global_seed(400 + i)
                        numbers = np.argsort(gnn_scores)[-7:] + 1
                        append_prediction(sorted([int(n) for sub in numbers for n in (sub if isinstance(sub, (list, np.ndarray)) else [sub])]), base_confidence=0.83)

            if self.bnn_model:
                from bnn_module import predict_bayesian_regression
                print("[INFO] BNNãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬ã‚’å®Ÿè¡Œä¸­")
            
                for i in range(num_candidates):
                    set_global_seed(random.randint(1000, 999999))  # ğŸ” æ¯å›ç•°ãªã‚‹ã‚·ãƒ¼ãƒ‰ã§äºˆæ¸¬
            
                    try:
                        bnn_preds = predict_bayesian_regression(
                            self.bnn_model,
                            self.bnn_guide,
                            X,
                            samples=1  # ğŸ” 1ã‚µãƒ³ãƒ—ãƒ«ãšã¤å€‹åˆ¥ç”Ÿæˆ
                        )
            
                        for pred in bnn_preds:
                            pred = np.array(pred).flatten()
                            numbers = np.round(pred).astype(int)
                            numbers = np.clip(numbers, 1, 37)
                            numbers = np.unique(numbers)
            
                            # å¿…è¦ãªã‚‰ä¸è¶³åˆ†ã‚’ãƒ©ãƒ³ãƒ€ãƒ è£œå®Œï¼ˆBNNã¯è¢«ã‚ŠãŒå‡ºã‚„ã™ã„ãŸã‚ï¼‰
                            while len(numbers) < 7:
                                add = random.randint(1, 37)
                                if add not in numbers:
                                    numbers = np.append(numbers, add)
            
                            numbers = np.sort(numbers[:7])  # å¿µã®ãŸã‚7å€‹åˆ¶é™
                            append_prediction(numbers, base_confidence=0.83)
            
                    except Exception as e:
                        print(f"[WARNING] BNNäºˆæ¸¬ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")

            print(f"[INFO] ç·äºˆæ¸¬å€™è£œæ•°ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«çµ±åˆï¼‰: {len(all_predictions)}ä»¶")
            numbers_only = [pred[0] for pred in all_predictions]
            confidence_scores = [pred[1] for pred in all_predictions]
            return numbers_only, confidence_scores

        except Exception as e:
            print(f"[ERROR] äºˆæ¸¬ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            traceback.print_exc()
            return None, None
        
def evaluate_predictions(predictions, actual_numbers):
    matches = []
    for pred in predictions:
        match_count = len(set(pred[0]) & set(actual_numbers))
        matches.append(match_count)
    return {
        'max_matches': max(matches),
        'avg_matches': np.mean(matches),
        'predictions_with_matches': list(zip(predictions, matches))
    }
# è¿½åŠ : æœ€æ–°ã®æŠ½ã›ã‚“æ—¥ã‚’å–å¾—ã™ã‚‹é–¢æ•°
official_url = "https://www.takarakuji-official.jp/ec/loto7/?kujiprdShbt=61&knyschm=0"

async def fetch_drawing_dates():
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(official_url, timeout=10) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    drawing_dates = []
                    date_elements = soup.select("dl.m_param.m_thumbSet_row")
                    for dl in date_elements:
                        dt_element = dl.find("dt", string="æŠ½ã›ã‚“æ—¥")
                        if dt_element:
                            dd_element = dt_element.find_next_sibling("dd")
                            if dd_element:
                                formatted_date = dd_element.text.strip().replace("/", "-")
                                drawing_dates.append(formatted_date)
                    
                    return drawing_dates
                else:
                    print(f"HTTPã‚¨ãƒ©ãƒ¼ {response.status}: {official_url}")
        except Exception as e:
            print(f"æŠ½ã›ã‚“æ—¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    return []

async def get_latest_drawing_dates():
    dates = await fetch_drawing_dates()
    return dates

def parse_number_string(number_str):
    """
    äºˆæ¸¬ç•ªå·ã‚„å½“é¸ç•ªå·ã®æ–‡å­—åˆ—ã‚’ãƒªã‚¹ãƒˆåŒ–ã™ã‚‹é–¢æ•°
    - ã‚¹ãƒšãƒ¼ã‚¹ / ã‚«ãƒ³ãƒ / ã‚¿ãƒ– åŒºåˆ‡ã‚Šã«å¯¾å¿œ
    - "07 15 20 28 29 34 36" â†’ [7, 15, 20, 28, 29, 34, 36]
    - "[7, 15, 20, 28, 29, 34, 36]" â†’ [7, 15, 20, 28, 29, 34, 36]
    """
    if pd.isna(number_str):
        return []  # NaN ã®å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
    
    # ä¸è¦ãªè¨˜å·ã‚’å‰Šé™¤ï¼ˆãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆï¼‰
    number_str = number_str.strip("[]").replace("'", "").replace('"', '')

    # ã‚¹ãƒšãƒ¼ã‚¹ãƒ»ã‚«ãƒ³ãƒãƒ»ã‚¿ãƒ–ã§åˆ†å‰²ã—ã€æ•´æ•°å¤‰æ›
    numbers = re.split(r'[\s,]+', number_str)

    # æ•°å­—ã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦æ•´æ•°å¤‰æ›
    return [int(n) for n in numbers if n.isdigit()]

def classify_rank(main_match, bonus_match):
    """æœ¬æ•°å­—ä¸€è‡´æ•°ã¨ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°ã‹ã‚‰Loto7ã®ç­‰ç´šã‚’åˆ¤å®š"""
    if main_match == 7:
        return "1ç­‰"
    elif main_match == 6 and bonus_match >= 1:
        return "2ç­‰"
    elif main_match == 6:
        return "3ç­‰"
    elif main_match == 5:
        return "4ç­‰"
    elif main_match == 4:
        return "5ç­‰"
    elif main_match == 3 and bonus_match >= 1:
        return "6ç­‰"
    else:
        return "è©²å½“ãªã—"
    
def calculate_precision_recall_f1(evaluation_df):
    y_true = []
    y_pred = []

    for _, row in evaluation_df.iterrows():
        actual = set(row["å½“é¸æœ¬æ•°å­—"])
        predicted = set(row["äºˆæ¸¬ç•ªå·"])
        for n in range(1, 38):
            y_true.append(1 if n in actual else 0)
            y_pred.append(1 if n in predicted else 0)

    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    print("\n=== è©•ä¾¡æŒ‡æ¨™ ===")
    print(f"Precision: {precision:.3f}")
    print(f"Recall:    {recall:.3f}")
    print(f"F1 Score:  {f1:.3f}")

def evaluate_prediction_accuracy_with_bonus(predictions_file="loto7_predictions.csv", results_file="loto7.csv"):
    """
    äºˆæ¸¬çµæœã¨å®Ÿéš›ã®å½“é¸çµæœã‚’æ¯”è¼ƒã—ã€ãƒœãƒ¼ãƒŠã‚¹æ•°å­—ã‚’è€ƒæ…®ã—ã¦ç²¾åº¦ã‚’è©•ä¾¡ã—ã€ç­‰ç´šã‚‚åˆ¤å®šã™ã‚‹
    """
    try:
        # === âœ… ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã‚„æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã§ã‚‚å®‰å…¨ã«èª­ã¿è¾¼ã‚€ ===
        try:
            predictions_df = pd.read_csv(predictions_file, encoding='utf-8-sig')
            if predictions_df.empty or predictions_df.shape[0] == 0 or "æŠ½ã›ã‚“æ—¥" not in predictions_df.columns:
                print(f"[WARNING] äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã‹ç„¡åŠ¹ã§ã™: {predictions_file}")
                return None
        except Exception as read_err:
            print(f"[WARNING] äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å¤±æ•—: {read_err}")
            return None

        results_df = pd.read_csv(results_file, encoding='utf-8-sig')
        evaluation_results = []

        for index, row in predictions_df.iterrows():
            draw_date = row["æŠ½ã›ã‚“æ—¥"]
            actual_row = results_df[results_df["æŠ½ã›ã‚“æ—¥"] == draw_date]
            if actual_row.empty:
                continue

            actual_numbers = parse_number_string(actual_row.iloc[0]["æœ¬æ•°å­—"])
            actual_bonus = parse_number_string(actual_row.iloc[0]["ãƒœãƒ¼ãƒŠã‚¹æ•°å­—"])

            for i in range(1, 6):  # äºˆæ¸¬1ã€œ5
                pred_col = f"äºˆæ¸¬{i}"
                if pred_col not in row or pd.isna(row[pred_col]):
                    continue

                try:
                    predicted_numbers = set(parse_number_string(row[pred_col]))
                    main_match = len(predicted_numbers & set(actual_numbers))
                    bonus_match = len(predicted_numbers & set(actual_bonus))
                    rank = classify_rank(main_match, bonus_match)

                    evaluation_results.append({
                        "æŠ½ã›ã‚“æ—¥": draw_date,
                        "äºˆæ¸¬ç•ªå·": list(predicted_numbers),
                        "å½“é¸æœ¬æ•°å­—": actual_numbers,
                        "å½“é¸ãƒœãƒ¼ãƒŠã‚¹": actual_bonus,
                        "æœ¬æ•°å­—ä¸€è‡´æ•°": main_match,
                        "ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°": bonus_match,
                        "ä¿¡é ¼åº¦": row.get(f"ä¿¡é ¼åº¦{i}", None),
                        "ç­‰ç´š": rank
                    })

                except Exception as e:
                    print(f"äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼ (è¡Œ {index}, äºˆæ¸¬ {i}): {e}")
                    continue

        evaluation_df = pd.DataFrame(evaluation_results)
        evaluation_df.to_csv("loto7_prediction_evaluation_with_bonus.csv", index=False, encoding='utf-8-sig')
        print("äºˆæ¸¬ç²¾åº¦ã®è©•ä¾¡çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: loto7_prediction_evaluation_with_bonus.csv")

        # çµ±è¨ˆå‡ºåŠ›
        print("\n=== äºˆæ¸¬ç²¾åº¦ã®çµ±è¨ˆæƒ…å ± ===")
        if not evaluation_df.empty:
            print(f"æœ€å¤§æœ¬æ•°å­—ä¸€è‡´æ•°: {evaluation_df['æœ¬æ•°å­—ä¸€è‡´æ•°'].max()}")
            print(f"å¹³å‡æœ¬æ•°å­—ä¸€è‡´æ•°: {evaluation_df['æœ¬æ•°å­—ä¸€è‡´æ•°'].mean():.2f}")
            print(f"æœ€å¤§ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°: {evaluation_df['ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°'].max()}")
            print(f"å¹³å‡ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°: {evaluation_df['ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°'].mean():.2f}")
            print("\n--- ç­‰ç´šã®åˆ†å¸ƒ ---")
            print(evaluation_df['ç­‰ç´š'].value_counts())
            # âœ… è©•ä¾¡æŒ‡æ¨™ã‚’è¿½åŠ ã§è¡¨ç¤º
            calculate_precision_recall_f1(evaluation_df)
        else:
            print("è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                # === è©•ä¾¡çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ› ===
        try:
            with open("loto7_evaluation_summary.txt", "w", encoding="utf-8") as f:
                f.write("=== äºˆæ¸¬ç²¾åº¦ã®çµ±è¨ˆæƒ…å ± ===\n")
                f.write(f"æœ€å¤§æœ¬æ•°å­—ä¸€è‡´æ•°: {evaluation_df['æœ¬æ•°å­—ä¸€è‡´æ•°'].max()}\n")
                f.write(f"å¹³å‡æœ¬æ•°å­—ä¸€è‡´æ•°: {evaluation_df['æœ¬æ•°å­—ä¸€è‡´æ•°'].mean():.2f}\n")
                f.write(f"æœ€å¤§ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°: {evaluation_df['ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°'].max()}\n")
                f.write(f"å¹³å‡ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°: {evaluation_df['ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°'].mean():.2f}\n\n")

                f.write("--- ç­‰ç´šã®åˆ†å¸ƒ ---\n")
                f.write(f"{evaluation_df['ç­‰ç´š'].value_counts().to_string()}\n\n")

                # Precision/Recall/F1
                y_true = []
                y_pred = []

                for _, row in evaluation_df.iterrows():
                    actual = set(row["å½“é¸æœ¬æ•°å­—"])
                    predicted = set(row["äºˆæ¸¬ç•ªå·"])
                    for n in range(1, 38):
                        y_true.append(1 if n in actual else 0)
                        y_pred.append(1 if n in predicted else 0)

                precision = precision_score(y_true, y_pred)
                recall = recall_score(y_true, y_pred)
                f1 = f1_score(y_true, y_pred)

                f.write("=== è©•ä¾¡æŒ‡æ¨™ ===\n")
                f.write(f"Precision: {precision:.3f}\n")
                f.write(f"Recall:    {recall:.3f}\n")
                f.write(f"F1 Score:  {f1:.3f}\n")
            print("è©•ä¾¡çµæœã‚’ loto7_evaluation_summary.txt ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"[WARNING] ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›å¤±æ•—: {e}")

        return evaluation_df

    except Exception as e:
        print(f"äºˆæ¸¬ç²¾åº¦ã®è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# äºˆæ¸¬çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹é–¢æ•°
def save_predictions_to_csv(predictions, drawing_date, filename="loto7_predictions.csv"):
    drawing_date = pd.to_datetime(drawing_date).strftime("%Y-%m-%d")
    row = {"æŠ½ã›ã‚“æ—¥": drawing_date}

    for i, (numbers, confidence) in enumerate(predictions[:5], 1):
        row[f"äºˆæ¸¬{i}"] = ', '.join(map(str, numbers))
        row[f"ä¿¡é ¼åº¦{i}"] = round(confidence, 3)

    df = pd.DataFrame([row])

    if os.path.exists(filename):
        try:
            existing_df = pd.read_csv(filename, encoding='utf-8-sig')
            if "æŠ½ã›ã‚“æ—¥" not in existing_df.columns:
                print(f"è­¦å‘Š: CSVã«'æŠ½ã›ã‚“æ—¥'åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
                existing_df = pd.DataFrame(columns=["æŠ½ã›ã‚“æ—¥"] + [f"äºˆæ¸¬{i}" for i in range(1, 6)] + [f"ä¿¡é ¼åº¦{i}" for i in range(1, 6)])
            existing_df = existing_df[existing_df["æŠ½ã›ã‚“æ—¥"] != drawing_date]
            df = pd.concat([existing_df, df], ignore_index=True)
        except Exception as e:
            print(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
            df = pd.DataFrame([row])

    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"äºˆæ¸¬çµæœã‚’ {filename} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

def is_running_with_streamlit():
    try:
        from streamlit.runtime.scriptrunner import get_script_run_ctx
        return get_script_run_ctx() is not None
    except ImportError:
        return False

def main_with_improved_predictions():
    set_global_seed(42)  # â˜…è¿½åŠ 
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    drawing_dates = asyncio.run(get_latest_drawing_dates())
    latest_drawing_date = drawing_dates[0] if drawing_dates else "ä¸æ˜"
    print("æœ€æ–°ã®æŠ½ã›ã‚“æ—¥:", latest_drawing_date)

    try:
        data = pd.read_csv("loto7.csv")
        data["æŠ½ã›ã‚“æ—¥"] = pd.to_datetime(data["æŠ½ã›ã‚“æ—¥"], errors='coerce')
        print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
    except Exception as e:
        print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return

    accuracy_results = evaluate_prediction_accuracy_with_bonus("loto7_predictions.csv", "loto7.csv")
    if accuracy_results is not None and not accuracy_results.empty:
        print("éå»ã®äºˆæ¸¬ç²¾åº¦ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚")

    X, _, _ = preprocess_data(data)
    input_size = X.shape[1] if X is not None else 10
    hidden_size = 128
    output_size = 7

    predictor = LotoPredictor(input_size, hidden_size, output_size)

    try:
        print("ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹...")
        predictor.train_model(data, accuracy_results=accuracy_results)
        print("ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å®Œäº†")
    except Exception as e:
        print(f"ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
        return

    if is_running_with_streamlit():
        st.title("ãƒ­ãƒˆ7äºˆæ¸¬AI")
        if st.button("äºˆæ¸¬ã‚’å®Ÿè¡Œ"):
            try:
                latest_data = data.tail(10)
                target_date = latest_data["æŠ½ã›ã‚“æ—¥"].max()
                history_data = data[data["æŠ½ã›ã‚“æ—¥"] < target_date]  # ğŸ”¥ æœªæ¥ãƒªãƒ¼ã‚¯é˜²æ­¢

                predictions, confidence_scores = predictor.predict(latest_data)

                if predictions is None:
                    print("[ERROR] äºˆæ¸¬ã«å¤±æ•—ã—ãŸãŸã‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
                    return

                verified_predictions = verify_predictions(list(zip(predictions, confidence_scores)), history_data)

                save_self_predictions(verified_predictions)

                for i, (numbers, confidence) in enumerate(verified_predictions[:5], 1):
                    st.write(f"äºˆæ¸¬ {i}: {numbers} (ä¿¡é ¼åº¦: {confidence:.3f})")

                save_predictions_to_csv(verified_predictions, latest_drawing_date)

            except Exception as e:
                st.error(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        print("[INFO] Streamlitä»¥å¤–ã®å®Ÿè¡Œç’°å¢ƒæ¤œå‡ºã€‚é€šå¸¸ã®ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ã§äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        try:
            latest_data = data.tail(10)
            target_date = latest_data["æŠ½ã›ã‚“æ—¥"].max()
            history_data = data[data["æŠ½ã›ã‚“æ—¥"] < target_date]  # ğŸ”¥ æœªæ¥ãƒªãƒ¼ã‚¯é˜²æ­¢

            predictions, confidence_scores = predictor.predict(latest_data)

            if predictions is None:
                print("[ERROR] äºˆæ¸¬ã«å¤±æ•—ã—ãŸãŸã‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
                return

            verified_predictions = verify_predictions(list(zip(predictions, confidence_scores)), history_data)

            save_self_predictions(verified_predictions)

            print("\n=== äºˆæ¸¬çµæœ ===")
            for i, (numbers, confidence) in enumerate(verified_predictions[:5], 1):
                print(f"äºˆæ¸¬ {i}: {numbers} (ä¿¡é ¼åº¦: {confidence:.3f})")

            save_predictions_to_csv(verified_predictions, latest_drawing_date)

        except Exception as e:
            print(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
    
def calculate_pattern_score(numbers, historical_data=None):
    score = 0
    odd_count = sum(1 for n in numbers if n % 2 != 0)
    if 2 <= odd_count <= 5:
        score += 1
    gaps = [numbers[i+1] - numbers[i] for i in range(len(numbers)-1)]
    if min(gaps) >= 2:
        score += 1
    total = sum(numbers)
    if 100 <= total <= 150:
        score += 1
    if max(numbers) - min(numbers) >= 15:
        score += 1
    return score

def plot_prediction_analysis(predictions, historical_data):
    plt.figure(figsize=(15, 10))
    
    # äºˆæ¸¬ç•ªå·ã®åˆ†å¸ƒ
    plt.subplot(2, 2, 1)
    all_predicted_numbers = [num for pred in predictions for num in pred[0]]
    plt.hist(all_predicted_numbers, bins=37, range=(1, 38), alpha=0.7)
    plt.title('äºˆæ¸¬ç•ªå·ã®åˆ†å¸ƒ')
    plt.xlabel('æ•°å­—')
    plt.ylabel('é »åº¦')
    
    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
    plt.subplot(2, 2, 2)
    confidence_scores = [pred[1] for pred in predictions]
    plt.hist(confidence_scores, bins=20, alpha=0.7)
    plt.title('ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ')
    plt.xlabel('ä¿¡é ¼åº¦')
    plt.ylabel('é »åº¦')
    
    # éå»ã®å½“é¸ç•ªå·ã¨ã®æ¯”è¼ƒ
    plt.subplot(2, 2, 3)
    historical_numbers = [num for numbers in historical_data['æœ¬æ•°å­—'] for num in numbers]
    plt.hist(historical_numbers, bins=37, range=(1, 38), alpha=0.5, label='éå»ã®å½“é¸')
    plt.hist(all_predicted_numbers, bins=37, range=(1, 38), alpha=0.5, label='äºˆæ¸¬')
    plt.title('äºˆæ¸¬ vs éå»ã®å½“é¸')
    plt.xlabel('æ•°å­—')
    plt.ylabel('é »åº¦')
    plt.legend()
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    plt.subplot(2, 2, 4)
    pattern_scores = [calculate_pattern_score(pred[0]) for pred in predictions]
    plt.scatter(range(len(pattern_scores)), pattern_scores, alpha=0.5)
    plt.title('äºˆæ¸¬ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚³ã‚¢')
    plt.xlabel('äºˆæ¸¬ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹')
    plt.ylabel('ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚³ã‚¢')
    
    plt.tight_layout()
    plt.savefig('prediction_analysis.png')
    plt.close()

def generate_evolution_graph(log_file="evolution_log.txt", output_file="evolution_graph.png"):
    """
    evolution_log.txtã‚’èª­ã¿è¾¼ã‚“ã§é€²åŒ–ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆãƒ»ä¿å­˜ã™ã‚‹
    """
    if not os.path.exists(log_file):
        print(f"[WARNING] é€²åŒ–ãƒ­ã‚° {log_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    dates = []
    counts = []

    with open(log_file, "r", encoding="utf-8") as f:
        for line in f:
            try:
                parts = line.strip().split(":")
                date_part = parts[0].strip()
                count_part = parts[2].strip()

                date = pd.to_datetime(date_part)
                count = int(count_part.split()[0])

                dates.append(date)
                counts.append(count)
            except Exception as e:
                print(f"[WARNING] ãƒ­ã‚°ãƒ‘ãƒ¼ã‚¹å¤±æ•—: {e}")
                continue

    if not dates:
        print("[WARNING] é€²åŒ–ãƒ­ã‚°ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # --- ã‚°ãƒ©ãƒ•æç”» ---
    plt.figure(figsize=(10, 6))
    plt.plot(dates, counts, marker='o', linestyle='-', color='blue')
    plt.title("è‡ªå·±é€²åŒ–å±¥æ­´ï¼ˆè‡ªå·±äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ä»¶æ•°æ¨ç§»ï¼‰")
    plt.xlabel("æ—¥æ™‚")
    plt.ylabel("è‡ªå·±äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ä»¶æ•°")
    plt.grid(True)
    plt.tight_layout()

    # --- ä¿å­˜ ---
    plt.savefig(output_file)
    plt.close()
    print(f"[INFO] é€²åŒ–å±¥æ­´ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")

def verify_predictions(predictions, historical_data, top_k=5):
    def check_number_constraints(numbers):
        """äºˆæ¸¬æ•°å­—é…åˆ—ã®åˆ¶ç´„ãƒã‚§ãƒƒã‚¯"""
        return (
            isinstance(numbers, (list, np.ndarray)) and
            len(numbers) == 7 and
            len(np.unique(numbers)) == 7 and
            np.all((np.array(numbers) >= 1) & (np.array(numbers) <= 37))
        )

    def get_high_match_templates(historical_df, match_threshold=6):
        """éå»ã®6æœ¬ä¸€è‡´ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æŠ½å‡º"""
        unique_sets = set()
        rows = historical_df['æœ¬æ•°å­—'].apply(lambda x: set(map(int, x)) if isinstance(x, list) else set())
        for i in range(len(rows)):
            for j in range(i + 1, len(rows)):
                intersect = rows[i] & rows[j]
                if len(intersect) >= match_threshold:
                    unique_sets.add(tuple(sorted(intersect)))
        return [set(t) for t in unique_sets]

    def penalize_overused_numbers(preds, threshold=0.05):
        """é »å‡ºæ•°å­—ã‚’å«ã‚€äºˆæ¸¬ã®ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã‚‹"""
        all_nums = [n for pred in preds for n in pred[0]]
        freq = pd.Series(all_nums).value_counts(normalize=True)
        penalized = []
        for nums, conf in preds:
            penalty = sum(freq.get(n, 0) > threshold for n in nums) * 0.1
            penalized.append((nums, conf * (1 - penalty)))
        return penalized

    print("[INFO] äºˆæ¸¬å€™è£œã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...")

    # --- æœ‰åŠ¹äºˆæ¸¬ã®ã¿æŠ½å‡º ---
    valid_predictions = [
        (np.sort(pred), conf)
        for pred, conf in predictions
        if check_number_constraints(pred)
    ]
    if not valid_predictions:
        print("[WARNING] æœ‰åŠ¹ãªäºˆæ¸¬ãŒã‚ã‚Šã¾ã›ã‚“")
        return []

    # --- é »å‡ºæ•°å­—ãƒšãƒŠãƒ«ãƒ†ã‚£ ---
    valid_predictions = penalize_overused_numbers(valid_predictions)

    # --- ä¿¡é ¼åº¦é †ã«100ä»¶ã¾ã§çµã‚‹ ---
    valid_predictions.sort(key=lambda x: x[1], reverse=True)
    candidates = valid_predictions[:100]

    # --- ã‚«ãƒãƒ¬ãƒƒã‚¸æœ€å¤§åŒ–ã§ top_k - 2 é¸æŠœ ---
    selected, used_numbers, used_flags = [], set(), [False] * len(candidates)
    while len(selected) < (top_k - 2):
        best_score, best_idx = -1, -1
        for idx, (nums, conf) in enumerate(candidates):
            if used_flags[idx]:
                continue
            combined = used_numbers | set(nums)
            coverage_score = len(combined)
            score = (coverage_score * 0.8) + (conf * 0.2)  # â¬… ã‚«ãƒãƒ¬ãƒƒã‚¸é‡è¦–ã«èª¿æ•´
            if score > best_score:
                best_score, best_idx = score, idx
        if best_idx == -1:
            break
        selected.append(candidates[best_idx])
        used_numbers.update(candidates[best_idx][0])
        used_flags[best_idx] = True

    # --- å¼·åˆ¶6æœ¬æ§‹æˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¿½åŠ ï¼ˆæœ€å¤§2ä»¶ï¼‰ ---
    try:
        print("[INFO] å¼·åˆ¶ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹æˆã®æ¢ç´¢ä¸­...")
        historical_data['æœ¬æ•°å­—'] = historical_data['æœ¬æ•°å­—'].apply(
            lambda x: list(map(int, x)) if isinstance(x, list) else []
        )
        templates = get_high_match_templates(historical_data)

        added = 0
        tried = set()
        while added < 2 and len(tried) < len(templates):
            base = set(random.choice(templates))
            if tuple(sorted(base)) in tried:
                continue
            tried.add(tuple(sorted(base)))

            available = list(set(range(1, 38)) - base)
            if len(base) >= 6 and available:
                base = set(random.sample(base, 6))  # 6å€‹ã ã‘æ®‹ã™
                base.add(random.choice(available))  # 1å€‹è¿½åŠ ã—ã¦7å€‹ã«
                combo = np.sort(list(base))
                selected.append((combo, 1.0))
                added += 1

        if added > 0:
            print(f"[INFO] å¼·åˆ¶6æœ¬æ§‹æˆã‚’ {added} ä»¶è¿½åŠ ã—ã¾ã—ãŸ")
        else:
            print("[INFO] å¼·åˆ¶ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ§‹æˆã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    except Exception as e:
        print(f"[WARNING] å¼·åˆ¶æ§‹æˆä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")

    print(f"[INFO] æœ€çµ‚é¸æŠã•ã‚ŒãŸäºˆæ¸¬æ•°: {len(selected)}")
    return selected


def extract_strong_features(evaluation_df, feature_df):
    """
    éå»äºˆæ¸¬è©•ä¾¡ã¨ç‰¹å¾´é‡ã‚’çµåˆã—ã€ã€Œæœ¬æ•°å­—ä¸€è‡´æ•°ã€ã¨ç›¸é–¢ã®é«˜ã„ç‰¹å¾´é‡ã‚’æŠ½å‡º
    """
    # ğŸ”’ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
    if evaluation_df is None or evaluation_df.empty:
        print("[WARNING] è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€é‡è¦ç‰¹å¾´é‡ã®æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return []

    if "æŠ½ã›ã‚“æ—¥" not in evaluation_df.columns:
        print("[WARNING] è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã« 'æŠ½ã›ã‚“æ—¥' åˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚é‡è¦ç‰¹å¾´é‡ã®æŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return []

    if feature_df is None or feature_df.empty or "æŠ½ã›ã‚“æ—¥" not in feature_df.columns:
        print("[WARNING] ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ã¾ãŸã¯ 'æŠ½ã›ã‚“æ—¥' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return []

    # ğŸ”§ æ—¥ä»˜å‹ã‚’æ˜ç¤ºçš„ã«æƒãˆã‚‹
    evaluation_df['æŠ½ã›ã‚“æ—¥'] = pd.to_datetime(evaluation_df['æŠ½ã›ã‚“æ—¥'], errors='coerce')
    feature_df['æŠ½ã›ã‚“æ—¥'] = pd.to_datetime(feature_df['æŠ½ã›ã‚“æ—¥'], errors='coerce')

    # â›“ çµåˆ
    merged = evaluation_df.merge(feature_df, on="æŠ½ã›ã‚“æ—¥", how="inner")
    if merged.empty:
        print("[WARNING] è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã¨ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã®çµåˆçµæœãŒç©ºã§ã™ã€‚")
        return []

    # ğŸ“Š ç›¸é–¢è¨ˆç®—
    correlations = {}
    for col in feature_df.columns:
        if col in ["æŠ½ã›ã‚“æ—¥", "æœ¬æ•°å­—", "ãƒœãƒ¼ãƒŠã‚¹æ•°å­—"]:
            continue
        try:
            if not np.issubdtype(merged[col].dtype, np.number):
                continue
            corr = np.corrcoef(merged[col], merged["æœ¬æ•°å­—ä¸€è‡´æ•°"])[0, 1]
            correlations[col] = abs(corr)
        except Exception:
            continue

    # ğŸ” ä¸Šä½5ç‰¹å¾´é‡ã‚’è¿”ã™
    top_features = sorted(correlations.items(), key=lambda x: -x[1])[:5]
    return [f[0] for f in top_features]

def reinforce_features(X, feature_names, important_features, multiplier=1.5):
    """
    æŒ‡å®šã•ã‚ŒãŸé‡è¦ç‰¹å¾´é‡ã‚’å¼·èª¿ï¼ˆå€¤ã‚’å€ç‡ã§å¢—å¼·ï¼‰
    """
    reinforced_X = X.copy()
    for feat in important_features:
        if feat in feature_names:
            idx = feature_names.index(feat)
            reinforced_X[:, idx] *= multiplier
    return reinforced_X

# --- ğŸ”¥ æ–°è¦è¿½åŠ é–¢æ•° ---
def extract_high_match_patterns(dataframe, min_match=6):
    """éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é«˜ä¸€è‡´ãƒ‘ã‚¿ãƒ¼ãƒ³ã ã‘æŠ½å‡º"""
    high_match_combos = []
    for idx1, row1 in dataframe.iterrows():
        nums1 = set(row1['æœ¬æ•°å­—'])
        for idx2, row2 in dataframe.iterrows():
            if idx1 >= idx2:
                continue
            nums2 = set(row2['æœ¬æ•°å­—'])
            if len(nums1 & nums2) >= min_match:
                high_match_combos.append(sorted(nums1))
    return high_match_combos

def calculate_number_frequencies(dataframe):
    """éå»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç•ªå·å‡ºç¾é »åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    all_numbers = [num for nums in dataframe['æœ¬æ•°å­—'] for num in nums]
    freq = pd.Series(all_numbers).value_counts().to_dict()
    return freq

def calculate_number_cycle_score(dataframe):
    """
    æ•°å­—ã”ã¨ã®æœªå‡ºç¾æœŸé–“ï¼ˆå‘¨æœŸï¼‰ã«åŸºã¥ã„ãŸã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    ç›´è¿‘ã§å‡ºã¦ã„ãªã„ã»ã©é«˜ã„ã‚¹ã‚³ã‚¢ã‚’ä»˜ã‘ã‚‹ã€‚
    """
    last_seen = {}
    today_index = len(dataframe)

    for idx, row in dataframe[::-1].iterrows():
        for number in row['æœ¬æ•°å­—']:
            if number not in last_seen:
                last_seen[number] = today_index - idx  # ä»Šä½•å›åˆ†å‰ã«å‡ºãŸã‹

    # æœ€å¤§æœªå‡ºå‘¨æœŸ = é«˜ã‚¹ã‚³ã‚¢ã¨ã™ã‚‹ãŸã‚åè»¢ï¼ˆä¾‹: æœªå‡ºæ—¥æ•°ãŒé•·ã„ã»ã©é«˜å¾—ç‚¹ï¼‰
    max_cycle = max(last_seen.values()) if last_seen else 1
    score = {n: last_seen.get(n, max_cycle) for n in range(1, 38)}
    return score

        
def bulk_predict_all_past_draws():
    set_global_seed(42)
    df = pd.read_csv("loto7.csv")
    df["æŠ½ã›ã‚“æ—¥"] = pd.to_datetime(df["æŠ½ã›ã‚“æ—¥"], errors='coerce')
    df = df.sort_values("æŠ½ã›ã‚“æ—¥").reset_index(drop=True)
    print("[INFO] æŠ½ã›ã‚“ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:", len(df), "ä»¶")

    pred_file = "loto7_predictions.csv"

    skip_dates = set()
    if os.path.exists(pred_file):
        try:
            pred_df = pd.read_csv(pred_file, encoding='utf-8-sig')
            if "æŠ½ã›ã‚“æ—¥" in pred_df.columns:
                skip_dates = set(pd.to_datetime(pred_df["æŠ½ã›ã‚“æ—¥"], errors='coerce').dropna().dt.strftime("%Y-%m-%d"))
        except Exception as e:
            print(f"[WARNING] äºˆæ¸¬ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        with open(pred_file, "w", encoding="utf-8-sig") as f:
            f.write("æŠ½ã›ã‚“æ—¥,äºˆæ¸¬1,ä¿¡é ¼åº¦1,äºˆæ¸¬2,ä¿¡é ¼åº¦2,äºˆæ¸¬3,ä¿¡é ¼åº¦3,äºˆæ¸¬4,ä¿¡é ¼åº¦4,äºˆæ¸¬5,ä¿¡é ¼åº¦5\n")

    predictor_cache = {}

    for i in range(10, len(df)):
        set_global_seed(1000 + i)

        test_date = df.iloc[i]["æŠ½ã›ã‚“æ—¥"]
        test_date_str = test_date.strftime("%Y-%m-%d")

        if test_date_str in skip_dates:
            print(f"[INFO] æ—¢ã«äºˆæ¸¬æ¸ˆã¿: {test_date_str} â†’ ã‚¹ã‚­ãƒƒãƒ—")
            continue

        print(f"\n=== {test_date_str} ã®äºˆæ¸¬ã‚’é–‹å§‹ ===")
        train_data = df.iloc[:i].copy()
        latest_data = df.iloc[i-10:i].copy()

        X, _, _ = preprocess_data(train_data)
        if X is None:
            print(f"[WARNING] {test_date_str} ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ã§ã™")
            continue

        input_size = X.shape[1]

        if i % 50 == 0 or input_size not in predictor_cache:
            print(f"[INFO] ãƒ¢ãƒ‡ãƒ«å†å­¦ç¿’: {test_date_str} æ™‚ç‚¹")
            predictor = LotoPredictor(input_size, 128, 7)
            predictor.train_model(train_data)
            predictor_cache[input_size] = predictor
        else:
            predictor = predictor_cache[input_size]

        predictions, confidence_scores = predictor.predict(latest_data)
        if predictions is None:
            print(f"[ERROR] {test_date_str} ã®äºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸ")
            continue

        verified_predictions = verify_predictions(list(zip(predictions, confidence_scores)), train_data)
        save_self_predictions(verified_predictions)
        save_predictions_to_csv(verified_predictions, test_date)
        git_commit_and_push("loto7_predictions.csv", "Auto update loto7_predictions.csv [skip ci]")

        model_dir = f"models/{test_date_str}"
        _save_all_models_no_self(predictor, model_dir)

        evaluate_prediction_accuracy_with_bonus("loto7_predictions.csv", "loto7.csv")

    # === ğŸ†• æœªæ¥1å›åˆ†ã®äºˆæ¸¬ã‚’è¿½åŠ  ===
    try:
        future_date = df["æŠ½ã›ã‚“æ—¥"].max() + pd.Timedelta(days=7)
        future_date_str = future_date.strftime("%Y-%m-%d")

        if future_date_str not in skip_dates:
            print(f"\n=== {future_date_str} ã®æœªæ¥äºˆæ¸¬ã‚’é–‹å§‹ ===")
            latest_data = df.tail(10).copy()
            train_data = df.copy()

            X, _, _ = preprocess_data(train_data)
            if X is None:
                print("[WARNING] æœªæ¥äºˆæ¸¬ç”¨ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒç„¡åŠ¹ã§ã™")
            else:
                input_size = X.shape[1]
                if input_size not in predictor_cache:
                    predictor = LotoPredictor(input_size, 128, 7)
                    predictor.train_model(train_data)
                    predictor_cache[input_size] = predictor
                else:
                    predictor = predictor_cache[input_size]

                predictions, confidence_scores = predictor.predict(latest_data)
                if predictions is not None:
                    verified_predictions = verify_predictions(list(zip(predictions, confidence_scores)), train_data)
                    save_self_predictions(verified_predictions)
                    save_predictions_to_csv(verified_predictions, future_date)
                    git_commit_and_push("loto7_predictions.csv", "Auto predict future draw [skip ci]")
                    print(f"[INFO] æœªæ¥äºˆæ¸¬ï¼ˆ{future_date_str}ï¼‰å®Œäº†")
        else:
            print(f"[INFO] æœªæ¥äºˆæ¸¬ï¼ˆ{future_date_str}ï¼‰ã¯æ—¢ã«å®Ÿè¡Œæ¸ˆã¿ã§ã™")

    except Exception as e:
        print(f"[WARNING] æœªæ¥äºˆæ¸¬ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        traceback.print_exc()

    print("\n=== ä¸€æ‹¬äºˆæ¸¬ã¨ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸ ===")

if __name__ == "__main__":
    import multiprocessing
    multiprocessing.set_start_method('spawn', force=True)
    bulk_predict_all_past_draws()


def log_prediction_summary(evaluation_df, log_path="prediction_accuracy_log.txt"):
    if evaluation_df is None or evaluation_df.empty:
        print("[WARNING] è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        return

    with open(log_path, "a", encoding="utf-8") as f:
        for _, row in evaluation_df.iterrows():
            date = row["æŠ½ã›ã‚“æ—¥"]
            match = row["æœ¬æ•°å­—ä¸€è‡´æ•°"]
            bonus = row["ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°"]
            rank = row["ç­‰ç´š"]
            confidence = row.get("ä¿¡é ¼åº¦", "N/A")
            pred = row["äºˆæ¸¬ç•ªå·"]

            f.write(f"{date}: ä¸€è‡´={match}æœ¬, ãƒœãƒ¼ãƒŠã‚¹={bonus}, ç­‰ç´š={rank}, ä¿¡é ¼åº¦={confidence}, ç•ªå·={pred}\n")

    print(f"[INFO] äºˆæ¸¬ç²¾åº¦å±¥æ­´ã‚’ {log_path} ã«è¿½è¨˜ã—ã¾ã—ãŸ")


# === å†å­¦ç¿’ã‚µãƒãƒªãƒ»é€²åŒ–ãƒ­ã‚°ï¼ˆè¿½è¨˜ï¼‰ ==========================================
import csv
from datetime import datetime
import subprocess

def _get_git_head():
    try:
        return subprocess.check_output(["git", "rev-parse", "HEAD"]).decode().strip()
    except Exception:
        return "N/A"

def summarize_evaluation(evaluation_df):
    """
    evaluate_prediction_accuracy_with_bonus(...) ãŒè¿”ã™ DataFrame ã‚’è¦ç´„ã—ã¦ dict ã‚’è¿”ã™
    æœŸå¾…åˆ—: ["æœ¬æ•°å­—ä¸€è‡´æ•°","ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°","ç­‰ç´š","å½“é¸æœ¬æ•°å­—","äºˆæ¸¬ç•ªå·"]
    """
    try:
        import pandas as pd  # ç¢ºå®Ÿã« pd ãŒã‚ã‚‹ã‚ˆã†ã«
    except Exception:
        pass
    if evaluation_df is None or len(evaluation_df) == 0:
        return None

    s = {}
    s["eval_rows"]        = int(len(evaluation_df))
    if "æœ¬æ•°å­—ä¸€è‡´æ•°" in evaluation_df.columns:
        s["max_main_match"]   = int(evaluation_df["æœ¬æ•°å­—ä¸€è‡´æ•°"].max())
        s["avg_main_match"]   = float(evaluation_df["æœ¬æ•°å­—ä¸€è‡´æ•°"].mean())
    if "ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°" in evaluation_df.columns:
        s["max_bonus_match"]  = int(evaluation_df["ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°"].max())
        s["avg_bonus_match"]  = float(evaluation_df["ãƒœãƒ¼ãƒŠã‚¹ä¸€è‡´æ•°"].mean())

    # ç­‰ç´šåˆ†å¸ƒ
    if "ç­‰ç´š" in evaluation_df.columns:
        counts = evaluation_df["ç­‰ç´š"].value_counts()
        for r in ["1ç­‰","2ç­‰","3ç­‰","4ç­‰","5ç­‰","6ç­‰","è©²å½“ãªã—"]:
            s[f"rank_{r}"] = int(counts.get(r, 0))

    # Precision / Recall / F1 ã‚’ç•ªå·ãƒ¬ãƒ™ãƒ«ã§ç®—å‡ºï¼ˆ0é™¤ç®—ã¯0æ‰±ã„ï¼‰
    y_true, y_pred = [], []
    if "å½“é¸æœ¬æ•°å­—" in evaluation_df.columns and "äºˆæ¸¬ç•ªå·" in evaluation_df.columns:
        for _, row in evaluation_df.iterrows():
            actual = set(row["å½“é¸æœ¬æ•°å­—"]) if isinstance(row["å½“é¸æœ¬æ•°å­—"], (list,set)) else set(row.get("å½“é¸æœ¬æ•°å­—_list", []))  # ä¿é™º
            predicted = set(row["äºˆæ¸¬ç•ªå·"]) if isinstance(row["äºˆæ¸¬ç•ªå·"], (list,set)) else set(row.get("äºˆæ¸¬ç•ªå·_list", []))
            # LOTO7 ã®ç•ªå·ç¯„å›²ï¼ˆ1..37ï¼‰ã«å¯¾ã—ã¦
            for n in range(1, 38):
                y_true.append(1 if n in actual else 0)
                y_pred.append(1 if n in predicted else 0)
        try:
            from sklearn.metrics import precision_score, recall_score, f1_score
            s["precision"] = float(precision_score(y_true, y_pred, zero_division=0))
            s["recall"]    = float(recall_score(y_true, y_pred, zero_division=0))
            s["f1"]        = float(f1_score(y_true, y_pred, zero_division=0))
        except Exception:
            s["precision"] = s["recall"] = s["f1"] = 0.0
    return s

def write_evolution_log(evaluation_df, data_max_date=None, seed=None, model_dir=None,
                        top_features=None, csv_path="logs/evolution.csv", json_dir="logs/evolution_detail"):
    """
    å†å­¦ç¿’1å›ã«ã¤ãCSVã«1è¡Œè¿½è¨˜ã—ã€åŒå†…å®¹ã‚’JSONã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã§ã‚‚ä¿å­˜
    """
    os.makedirs(os.path.dirname(csv_path), exist_ok=True)
    os.makedirs(json_dir, exist_ok=True)

    summary = summarize_evaluation(evaluation_df) or {}
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    head = _get_git_head()

    row = {
        "timestamp": now,
        "data_max_date": str(data_max_date) if data_max_date is not None else "N/A",
        "seed": seed if seed is not None else "",
        "model_dir": model_dir if model_dir is not None else "",
        "git_head": head,
        **summary
    }
    if top_features:
        row["top_features"] = "|".join(map(str, top_features))

    # CSVè¿½è¨˜ï¼ˆãƒ˜ãƒƒãƒ€è‡ªå‹•ï¼‰
    write_header = not os.path.exists(csv_path)
    header = list(row.keys())
    with open(csv_path, "a", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=header)
        if write_header:
            w.writeheader()
        w.writerow(row)

    # JSONã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
    json_name = datetime.now().strftime("%Y%m%d_%H%M%S") + ".json"
    with open(os.path.join(json_dir, json_name), "w", encoding="utf-8") as jf:
        import json as _json
        _json.dump(row, jf, ensure_ascii=False, indent=2)

    print(f"[LOG] å†å­¦ç¿’ã‚µãƒãƒªã‚’ä¿å­˜: {csv_path} / {json_name}")
    return row

def generate_evolution_graph_from_csv(csv_path="logs/evolution.csv", metric="f1", output_file="logs/evolution_graph.png"):
    """
    evolution.csvï¼ˆwrite_evolution_logã§ä½œæˆï¼‰ã‹ã‚‰ metric ã®æ¨ç§»ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
    """
    try:
        import pandas as _pd
        import matplotlib.pyplot as plt
    except Exception as e:
        print("[WARNING] å¯è¦–åŒ–ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿ã«å¤±æ•—:", e)
        return

    if not os.path.exists(csv_path):
        print(f"[WARNING] é€²åŒ–CSV {csv_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    df = _pd.read_csv(csv_path, encoding="utf-8")
    if "timestamp" not in df.columns or metric not in df.columns:
        print(f"[WARNING] CSVã«å¿…è¦ãªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“: timestamp / {metric}")
        return

    df["timestamp"] = _pd.to_datetime(df["timestamp"], errors="coerce")
    df = df.sort_values("timestamp")
    plt.figure(figsize=(10,6))
    plt.plot(df["timestamp"], df[metric], marker="o")
    plt.title(f"è‡ªå·±é€²åŒ–å±¥æ­´ï¼ˆ{metric} æ¨ç§»ï¼‰")
    plt.xlabel("æ—¥æ™‚")
    plt.ylabel(metric.upper())
    plt.grid(True)
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    plt.tight_layout()
    plt.savefig(output_file)
    plt.close()
    print(f"[LOG] é€²åŒ–ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {output_file}")
# === /å†å­¦ç¿’ã‚µãƒãƒªãƒ»é€²åŒ–ãƒ­ã‚° ã“ã“ã¾ã§ ==========================================

def evaluate_prediction_accuracy_with_bonus_compat(*args, **kwargs):
    """Compatibility shim.
    Older code may call this function with various parameters like:
      - evaluate_prediction_accuracy_with_bonus_compat(prediction_file=..., output_csv=..., output_txt=...)
      - evaluate_prediction_accuracy_with_bonus_compat(predictions_file=..., results_file=...)
      - evaluate_prediction_accuracy_with_bonus_compat(pred_file, res_file)
    This wrapper maps to evaluate_prediction_accuracy_with_bonus(predictions_file, results_file).
    Any extra arguments are ignored.
    """
    # Try to extract the two essential file paths from kwargs or positional args.
    predictions_file = kwargs.get("prediction_file") or kwargs.get("predictions_file")
    results_file = kwargs.get("results_file") or kwargs.get("result_file")
    # Fallback to positional args
    if predictions_file is None and len(args) >= 1:
        predictions_file = args[0]
    if results_file is None and len(args) >= 2:
        results_file = args[1]
    # Final fallback: common defaults
    if predictions_file is None:
        predictions_file = "loto7_predictions.csv"
    if results_file is None:
        results_file = "loto7.csv"
    return evaluate_prediction_accuracy_with_bonus(predictions_file, results_file)
